# -*- coding: utf-8 -*-
# ---
# jupyter:
#   jupytext:
#     cell_metadata_filter: title,-all
#     custom_cell_magics: kql
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.11.2
#   kernelspec:
#     display_name: AVE
#     language: python
#     name: python3
# ---

# %% [markdown]
# # Stress-testing our results
# At this point, we've shown a lot of cool results, but qualitative data
# is fickle and subject to both selection effects and confirmation bias.
# In this notebook, we perform a set of qualitative stress-tests.

# %%
try:
    import algebraic_value_editing
except ImportError:
    commit = "15bcf55"  # Stable commit
    get_ipython().run_line_magic(  # type: ignore
        magic_name="pip",
        line=(
            "install -U"
            f" git+https://github.com/montemac/algebraic_value_editing.git@{commit}"
        ),
    )



# %%
import torch
import pandas as pd
from typing import List, Callable, Dict, Tuple
from jaxtyping import Float

import plotly.express as px
import plotly.graph_objects as go
import numpy as np


from transformer_lens.HookedTransformer import HookedTransformer

from algebraic_value_editing import hook_utils, prompt_utils, completion_utils
from algebraic_value_editing.prompt_utils import RichPrompt



# %%
device: str = "cuda:2"  # TODO update for colab
model_name = "gpt2-xl"
model: HookedTransformer = HookedTransformer.from_pretrained(model_name, device="cpu")
_ = model.to(device)

_ = torch.set_grad_enabled(False)
torch.manual_seed(0)  # For reproducibility



# %% [markdown]
# ## Measuring the magnitudes of the steering vectors at each residual stream position
# How "big" are our edits, compared to the normal activations? Let's first
# examine what the residual stream magnitudes tend to be, by taking the L2
# norm of the residual stream at each sequence position. We'll do this for
# a range of prompts at a range of locations in the forward pass.
#
# (Most of the below prompts were generated by GPT4.)

# %%
prompts: List[str] = [
    "Bush did 9/11 because",
    "Barack Obama was born in",
    "Shrek starts off in a swamp",
    "I went up to my friend and said",
    "I talk about weddings constantly",
    "I bring up weddings in every situation",
    (
        "I hate talking about weddings. Instead, let's talk about a totally"
        " different topic, like the impact of NGDP on the price of gold."
    ),
    (
        "Artificial intelligence is transforming industries and reshaping the"
        " way we live, work, and interact."
    ),
    (
        "Climate change is one of the most pressing issues of our time, and we"
        " must take immediate action to reduce our carbon footprint."
    ),
    (
        "The rise of electric vehicles has led to an increased demand for"
        " lithium-ion batteries, driving innovation in the field of battery"
        " technology."
    ),
    (
        "The blockchain technology has the potential to revolutionize"
        " industries such as finance, supply chain management, and digital"
        " identity verification."
    ),
    (
        "CRISPR-Cas9 is a groundbreaking gene editing technology that allows"
        " scientists to make precise changes to an organism's DNA."
    ),
    (
        "Quantum computing promises to solve problems that are currently"
        " intractable for classical computers, opening up new frontiers in"
        " fields like cryptography and materials science."
    ),
    (
        "Virtual reality and augmented reality are transforming the way we"
        " experience and interact with digital content."
    ),
    (
        "3D printing is revolutionizing manufacturing, enabling the creation"
        " of complex and customized products on demand."
    ),
    (
        "The Internet of Things (IoT) is connecting everyday objects to the"
        " internet, providing valuable data and insights for businesses and"
        " consumers."
    ),
    (
        "Machine learning algorithms are becoming increasingly sophisticated,"
        " enabling computers to learn from data and make predictions with"
        " unprecedented accuracy."
    ),
    (
        "Renewable energy sources like solar and wind power are essential for"
        " reducing greenhouse gas emissions and combating climate change."
    ),
    (
        "The development of autonomous vehicles has the potential to greatly"
        " improve safety and efficiency on our roads."
    ),
    (
        "The human microbiome is a complex ecosystem of microbes living in and"
        " on our bodies, and its study is shedding new light on human health"
        " and disease."
    ),
    (
        "The use of drones for delivery, surveillance, and agriculture is"
        " rapidly expanding, with many companies investing in drone"
        " technology."
    ),
    (
        "The sharing economy, powered by platforms like Uber and Airbnb, is"
        " disrupting traditional industries and changing the way people access"
        " goods and services."
    ),
    (
        "Deep learning is a subset of machine learning that uses neural"
        " networks to model complex patterns in data."
    ),
    (
        "The discovery of exoplanets has fueled the search for"
        " extraterrestrial life and advanced our understanding of planetary"
        " systems beyond our own."
    ),
    (
        "Nanotechnology is enabling the development of new materials and"
        " devices at the atomic and molecular scale."
    ),
    (
        "The rise of big data is transforming industries, as companies seek to"
        " harness the power of data analytics to gain insights and make better"
        " decisions."
    ),
    (
        "Advancements in robotics are leading to the development of robots"
        " that can perform complex tasks and interact with humans in natural"
        " ways."
    ),
    (
        "The gig economy is changing the nature of work, as more people turn"
        " to freelancing and contract work for flexibility and autonomy."
    ),
    (
        "The Mars rover missions have provided valuable data on the geology"
        " and climate of the Red Planet, paving the way for future manned"
        " missions."
    ),
    (
        "The development of 5G networks promises faster and more reliable"
        " wireless connectivity, enabling new applications in areas like IoT"
        " and smart cities."
    ),
    (
        "Gene therapy offers the potential to treat genetic diseases by"
        " replacing, modifying, or regulating specific genes."
    ),
    (
        "The use of facial recognition technology raises important questions"
        " about privacy, surveillance, and civil liberties."
    ),
    (
        "Precision agriculture uses data and technology to optimize crop"
        " yields and reduce environmental impacts."
    ),
    (
        "Neuromorphic computing aims to develop hardware that mimics the"
        " structure and function of the human brain."
    ),
    (
        "Breaking news: Local man wins the lottery and plans to donate half of"
        " his earnings to charity"
    ),
    (
        "How to grow your own organic vegetables in your backyard â€“ step by"
        " step guide"
    ),
    (
        "omg I can't believe this new phone has such a terrible battery life,"
        " it doesn't even last a full day!"
    ),
    "Top 10 travel destinations you must visit before you die",
    "What are the best ways to invest in cryptocurrency?",
    (
        "I've been using this acne cream for a month and it's only making my"
        " skin worse, anyone else having this issue?"
    ),
    ("The secret to a happy and healthy relationship is communication and" " trust"),
    "Rumor has it that the famous celebrity couple is getting a divorce",
    (
        "I recently switched to a vegan diet and I feel so much better, I"
        " can't believe I didn't do it sooner"
    ),
    "Can someone help me with my math homework? I'm stuck on this problem...",
    (
        "UFO sightings have increased in the past few years, are we close to"
        " making contact with extraterrestrial life?"
    ),
    (
        "The government is hiding the truth about climate change and how it's"
        " affecting our planet"
    ),
    "Are video games causing violence among teenagers? A new study says yes",
    "A new study reveals the benefits of drinking coffee every day",
    "lol this new meme is hilarious, I can't stop laughing!",
    (
        "I'm so tired of people arguing about politics on the internet, can't"
        " we all just get along?"
    ),
    (
        "I love this new TV show, the characters are so well-developed and the"
        " plot is amazing"
    ),
    ("A devastating earthquake hit the city last night, leaving thousands" " homeless"),
    "Scientists discover a new species of fish deep in the ocean",
    "Why are people still believing in flat earth theory?",
    (
        "The local animal shelter is holding an adoption event this weekend,"
        " don't miss it!"
    ),
    (
        "The city is planning to build a new park in the neighborhood,"
        " residents are excited"
    ),
    "My dog ate my homework, literally, can anyone relate?",
    (
        "This new diet trend is taking the world by storm, but is it really"
        " effective?"
    ),
]



# %%
DF_COLS: List[str] = [
    "Prompt",
    "Activation Location",
    "Activation Name",
    "Magnitude",
]

sampling_kwargs: Dict[str, float] = {
    "temperature": 1.0,
    "top_p": 0.3,
    "freq_penalty": 1.0,
}

num_layers: int = model.cfg.n_layers


# %% [markdown]
# Now let's plot how the steering vector magnitudes change with layer
# number. These magnitudes are the Frobenius norms of the net activation
# vectors (adding residual streams "Anger" and subtracting the
# residual streams for "Calm"). Let's sanity-check that the magnitudes
# look reasonable, given what we **[know about how residual stream norm
# increases exponentially during forward passes]().** TODO add link


# %%
def line_plot(
    df: pd.DataFrame,
    log_y: bool = True,
    title: str = "Residual Stream Magnitude by Layer Number",
    legend_title_text: str = "Prompt",
) -> go.Figure:
    """Make a line plot of the RichPrompt magnitudes. If log_y is True,
    adds a column to the dataframe with the log10 of the magnitude."""
    for col in ["Prompt", "Activation Location", "Magnitude"]:
        assert col in df.columns, f"Column {col} not in dataframe"

    if log_y:
        df["LogMagnitude"] = np.log10(df["Magnitude"])

    fig = px.line(
        df,
        x="Activation Location",
        y="LogMagnitude" if log_y else "Magnitude",
        color="Prompt",
        color_discrete_sequence=px.colors.sequential.Rainbow[::-1],
    )

    fig.update_layout(
        legend_title_text=legend_title_text,
        title=title,
        xaxis_title="Layer Number",
        yaxis_title=f"Magnitude{' (log 10)' if log_y else ''}",
    )

    return fig



# %%
def steering_magnitudes_dataframe(
    model: HookedTransformer,
    act_adds: List[RichPrompt],
    locations: List[int],
) -> pd.DataFrame:
    """Compute the relative magnitudes of the steering vectors at the
    locations in the model."""
    steering_df = pd.DataFrame(columns=DF_COLS)

    for act_loc in locations:
        relocated_adds: List[RichPrompt] = [
            RichPrompt(prompt=act_add.prompt, coeff=act_add.coeff, act_name=act_loc)
            for act_add in act_adds
        ]
        mags: torch.Tensor = hook_utils.steering_vec_magnitudes(
            model=model, act_adds=relocated_adds
        ).cpu()

        prompt1_toks, prompt2_toks = [
            model.to_str_tokens(addition.prompt) for addition in relocated_adds
        ]

        for pos, mag in enumerate(mags):
            tok1, tok2 = prompt1_toks[pos], prompt2_toks[pos]
            row = pd.DataFrame(
                {
                    "Prompt": [f"{tok1}-{tok2}, pos {pos}"],
                    "Activation Location": [act_loc],
                    "Magnitude": [mag],
                }
            )

            # Append the new row to the dataframe
            steering_df = pd.concat([steering_df, row], ignore_index=True)

    return steering_df



# %% Make a plotly line plot of the steering vector magnitudes
anger_calm_additions: List[RichPrompt] = [
    RichPrompt(prompt="Anger", coeff=1, act_name=0),
    RichPrompt(prompt="Calm", coeff=-1, act_name=0),
]
all_resid_pre_locations: List[int] = torch.arange(0, num_layers, 1).tolist()

steering_df: pd.DataFrame = steering_magnitudes_dataframe(
    model=model,
    act_adds=anger_calm_additions,
    locations=all_resid_pre_locations,
)

fig: go.Figure = line_plot(steering_df)
fig.show()



# %% [markdown]
# These steering vector magnitudes also increase exponentially with
# layer number. This is in line with our previous results.
#
# The steering vector's 0 position `<|endoftext|>` - `<|endoftext|>` magnitude is always 0,
# because it's the zero vector. Thus, its relative magnitude is also 0.


# %% Let's plot the steering vector magnitudes against the prompt
def relative_magnitudes_dataframe(
    model: HookedTransformer,
    act_adds: List[RichPrompt],
    prompt: str,
    locations: List[int],
) -> pd.DataFrame:
    """Compute the relative magnitudes of the steering vectors at the
    locations in the model."""
    relative_df = pd.DataFrame(columns=DF_COLS)

    for act_loc in locations:
        relocated_adds: List[RichPrompt] = [
            RichPrompt(prompt=act_add.prompt, coeff=act_add.coeff, act_name=act_loc)
            for act_add in act_adds
        ]
        mags: torch.Tensor = hook_utils.steering_magnitudes_relative_to_prompt(
            model=model, prompt=prompt, act_adds=relocated_adds
        ).cpu()

        prompt1_toks, prompt2_toks = [
            model.to_str_tokens(addition.prompt) for addition in relocated_adds
        ]

        for pos, mag in enumerate(mags):
            tok1, tok2 = prompt1_toks[pos], prompt2_toks[pos]
            row = pd.DataFrame(
                {
                    "Prompt": [f"{tok1}-{tok2}, pos {pos}"],
                    "Activation Location": [act_loc],
                    "Magnitude": [mag],
                }
            )

            # Append the new row to the dataframe
            relative_df = pd.concat([relative_df, row], ignore_index=True)

    return relative_df



# %% Make a line plot of the relative steering vector magnitudes
anger_calm_additions: List[RichPrompt] = [
    RichPrompt(prompt="Anger", coeff=1, act_name=0),
    RichPrompt(prompt="Calm", coeff=-1, act_name=0),
]
relative_df: pd.DataFrame = relative_magnitudes_dataframe(
    model=model,
    act_adds=anger_calm_additions,
    prompt="I think you're",
    locations=all_resid_pre_locations,
)

fig: go.Figure = line_plot(
    relative_df,
    log_y=False,
    legend_title_text="Residual stream",
    title="Positionwise (Steering Vector Magnitude) / (Prompt Magnitude)",
)

# Add a subtitle
fig.update_layout(
    annotations=[
        go.layout.Annotation(
            text='Prompt: "I think you\'re"',
            showarrow=False,
            xref="paper",
            yref="paper",
            x=0.08,
            y=1.015,
            xanchor="center",
            yanchor="bottom",
            font=dict(size=13),
        )
    ],
    width=800,
)
fig.show()



# %% [markdown]
# We don't know why the relative magnitude decreases during the forward
# pass.

# %% [markdown]
# Great, so there are reasonable relative magnitudes of the `Anger` -
# `Calm` steering vector.
# Is this true for other vectors? Some vectors, like `_anger` - `_calm`,
# have little qualitative impact. Maybe they're low-norm?

# %%
anger_calm_additions: List[RichPrompt] = [
    RichPrompt(prompt=" anger", coeff=1, act_name=0),
    RichPrompt(prompt=" calm", coeff=-1, act_name=0),
]
relative_df: pd.DataFrame = relative_magnitudes_dataframe(
    model=model,
    act_adds=anger_calm_additions,
    prompt="I think you're",
    locations=all_resid_pre_locations,
)

fig: go.Figure = line_plot(
    relative_df,
    log_y=False,
    legend_title_text="Residual stream",
    title="Positionwise Steering Vector Magnitude / Prompt Magnitude",
)

# Add a subtitle
fig.update_layout(
    annotations=[
        go.layout.Annotation(
            text='Prompt: "I think you\'re"',
            showarrow=False,
            xref="paper",
            yref="paper",
            x=0.06,
            y=1.015,
            xanchor="center",
            yanchor="bottom",
            font=dict(size=13),
        )
    ]
)
fig.show()



# %% [markdown]
# Nope, `_anger` â€“ `_calm` has reasonable magnitude. So that isn't why it
# has little qualitative impact.

# %% [markdown]
# ## Injecting similar-magnitude random vectors
# Let's try injecting random vectors with similar magnitudes to the
# steering vectors. If GPT2XL is mostly robust to this addition, this
# suggests the presence of lots of tolerance to internal noise, and seems like
# a tiny bit of evidence of superposition (since a bunch of
# not-quite-orthogonal features will noisily unembed, and the model has
# to be performant in the face of this).
#
# But mostly, it just seems like a
# good additional data point to have.
#
# We're going to draw a random vector from a normal distribution with
# mean 0 and standard deviation 1. We'll then scale it to have the same
# magnitude as the `Anger`â€“`Calm` steering vector, and show the
# qualitative results.

# %%
# Get the steering vector magnitudes for the anger-calm steering vector
# at layer 6
anger_calm_additions_10: List[RichPrompt] = [
    RichPrompt(prompt="Anger", coeff=10, act_name=20),
    RichPrompt(prompt="Calm", coeff=-10, act_name=20),
]
num_anger_completions: int = 5
anger_vec: Float[torch.Tensor, "batch seq d_model"] = hook_utils.get_prompt_activations(
    model, anger_calm_additions_10[0]
) + hook_utils.get_prompt_activations(model, anger_calm_additions_10[1])



# %%
mags: torch.Tensor = hook_utils.steering_vec_magnitudes(
    model=model, act_adds=anger_calm_additions_10
).cpu()

rand_act: Float[torch.Tensor, "seq d_model"] = torch.randn(size=[len(mags), 1600])

# Rescale appropriately
scaling_factors: torch.Tensor = mags / rand_act.norm(dim=1)
rand_act = rand_act * scaling_factors[:, None]
rand_act[0, :] = 0  # Zero out the first token

print("Checking for similar magnitudes between steering vector and random" " vector:")
print(
    f"Steering vector magnitudes: {mags}\nRandom vector magnitudes:"
    f" {rand_act.norm(dim=1)}\n"
)

# Compare maximum magnitude of steering vector to maximum magnitude of
# random vector
print(f"Max steering vector value: {anger_vec.max():.1f}")
print(f"Max random vector value: {rand_act.max():.1f}")
rand_act = rand_act.unsqueeze(0)  # Add a batch dimension



# %%
# Get the model device so we can move rand_act off of the cpu
model_device: torch.device = next(model.parameters()).device
# Get the hook function
rand_hook: Callable = hook_utils.hook_fn_from_activations(
    activations=rand_act.to(model_device)
)
act_name: str = prompt_utils.get_block_name(block_num=20)
rand_hooks: Dict[str, Callable] = {act_name: rand_hook}

anger_prompts: List[str] = [
    "I think you're",
    "Shrek starts off with a scene about",
]
for prompt in anger_prompts:
    normal_df = completion_utils.gen_using_hooks(
        model=model,
        prompt_batch=[prompt] * num_anger_completions,
        hook_fns={},
        tokens_to_generate=60,
        seed=0,
        **sampling_kwargs,
    )
    print("\n")
    rand_df = completion_utils.gen_using_hooks(
        model=model,
        prompt_batch=[prompt] * num_anger_completions,
        hook_fns=rand_hooks,
        tokens_to_generate=60,
        seed=0,
        **sampling_kwargs,
    )
    completion_utils.pretty_print_completions(
        pd.concat([normal_df, rand_df], ignore_index=True),
    )



# %% [markdown]
# The random vector injection seems to not have much effect. We infer that GPT-2-XL
# is somewhat resistant to generic random intervention,
# and is instead controllable through consistent feature directions
# which are added to its forward pass by steering vectors.

# %% [markdown]
# ## A "random text" steering vector produces strange results
# However, the results are still syntactically coherent.

# %%
nonsense_vector: List[RichPrompt] = [
    *prompt_utils.get_x_vector(
        prompt1="fdsajl; fs",
        prompt2="",
        coeff=10,
        act_name=20,
        model=model,
        pad_method="tokens_right",
        custom_pad_id=int(model.to_single_token(" ")),
    )
]

# Let's make sure the nonsense vector is the same scale as the anger vector
anger_mags: torch.Tensor = hook_utils.steering_vec_magnitudes(
    model=model, act_adds=anger_calm_additions_10
).cpu()

nonsense_mags: torch.Tensor = hook_utils.steering_vec_magnitudes(
    model=model, act_adds=nonsense_vector
).cpu()

# Get average ratio between non-EOS anger and nonsense tokens
scaling_factor: float = anger_mags[1:].mean().item() / nonsense_mags[1:].mean().item()

rescaled_nonsense_vector: List[RichPrompt] = [
    *prompt_utils.get_x_vector(
        prompt1="fdsajl; fs",
        prompt2="",
        coeff=10 * scaling_factor,
        act_name=20,
        model=model,
        pad_method="tokens_right",
        custom_pad_id=int(model.to_single_token(" ")),
    )
]

# See how the model responds to the nonsense vector
completion_utils.print_n_comparisons(
    model=model,
    prompt="I went up to my friend and said",
    rich_prompts=rescaled_nonsense_vector,
    num_comparisons=5,
    **sampling_kwargs,
)



# %% [markdown]
# The model isn't very affected by the properly scaled nonsense vector.
# However, very large nonsense vectors do affect the model:

# %%
large_nonsense_vector: List[RichPrompt] = [
    *prompt_utils.get_x_vector(
        prompt1="fdsajl; fs",
        prompt2="",
        coeff=1000,
        act_name=20,
        model=model,
        pad_method="tokens_right",
        custom_pad_id=int(model.to_single_token(" ")),
    )
]

completion_utils.print_n_comparisons(
    model=model,
    prompt="I went up to my friend and said",
    rich_prompts=large_nonsense_vector,
    num_comparisons=5,
    **sampling_kwargs,
)



# %% [markdown]
# # Testing the hypothesis that we're "basically injecting extra tokens"
# There's a hypothesis that the steering vectors are just injecting
# extra tokens into the forward pass. In some situations, this makes
# sense. Given prompt "I love you because", if we inject a `_wedding` token at position 1 with large
# coefficient, perhaps the model just "sees" the sentence "_wedding love
# you because".
#
# However, in general, it's not clear what this hypothesis means. Tokens
# are a discrete quantity. You can't have more than one in a single
# position. You can't have three times `_wedding` and then negative
# three times `_` (space), on top of `I`. That's just not a thing which
# can be done using "just tokens."
#
# Even though this hypothesis isn't strictly true, there are still
# interesting versions to investigate. For example, consider the
# steering vector formed by adding `Anger` and subtracting `Calm` at
# layer 20, with coefficient 10. Perhaps what matters is not so much the
# "cognitive work" done by transformer blocks 0 through 19, but the
# 10*(embed(`Anger`) - embed(`Calm`)) vector. (As pointed out by the [mathematical
# framework for transformer
# circuits](https://transformer-circuits.pub/2021/framework/index.html),
# this is a component of the `Anger`-`Calm` steering vector.)
#
# We test this hypothesis by recording the relevant embedding
# vector, and then hooking in to the model at layer 20 to add the embedding vector
# to the forward pass.
#
# Suppose that this intervention also makes GPT-2-XL output completions
# with an angry sentiment, while preserving coherence. This result would be evidence that a lot
# of the
# steering vector's effect from the embedding vector, and not from the
# other computational work done by blocks 0â€“19.
#
# However, if the intervention doesn't make GPT-2-XL output particularly angry
# completions, then this is evidence that the `Anger`â€“`Calm` steering
# vector's effect is
# mostly from the computational work done by blocks 0â€“19.


# %%
def hooks_source_to_target(
    model: HookedTransformer,
    act_adds: List[RichPrompt],
    target_block: int,
    source_block: int = 0,
) -> Dict[str, Callable]:
    """Record the net steering vector at `source_block` for the prompts
    and coefficients given by `RichPrompts`, and return a dictionary with a hook which adds
    the activations at `target_block`."""
    for block_num in (source_block, target_block):
        assert (
            0 <= block_num <= model.cfg.n_layers
        ), f"block_num must be between 0 and {model.n_layers}, inclusive."

    source_adds: List[RichPrompt] = [
        RichPrompt(prompt=act_add.prompt, coeff=act_add.coeff, act_name=source_block)
        for act_add in act_adds
    ]

    # Make a dictionary of hooks which add to the activations at the
    # source layer
    embed_dict: Dict[str, Callable] = hook_utils.hook_fns_from_rich_prompts(
        model=model, rich_prompts=source_adds
    )

    # We want to add at the target layer d
    target_dict: Dict[str, Callable] = {
        prompt_utils.get_block_name(block_num=target_block): embed_dict[
            prompt_utils.get_block_name(block_num=source_block)
        ]
    }
    return target_dict



# %%
# Get the hooks for the steering vector at layer 0
anger_calm_additions_10: List[RichPrompt] = [
    RichPrompt(prompt="Anger", coeff=10, act_name=20),
    RichPrompt(prompt="Calm", coeff=-10, act_name=20),
]

transplant_hooks_0: Dict[str, Callable] = hooks_source_to_target(
    model=model,
    act_adds=anger_calm_additions_10,
    target_block=20,
    source_block=0,
)
anger_hooks: Dict[str, Callable] = hooks_source_to_target(
    model=model,
    act_adds=anger_calm_additions_10,
    target_block=20,
    source_block=20,
)

# Run the model with these hooks
transplant_df_0, normal_df = [
    completion_utils.gen_using_hooks(
        model=model,
        prompt_batch=["I think you're a"] * 15,
        hook_fns=hooks,
        seed=0,
        **sampling_kwargs,
    )
    for hooks in (transplant_hooks_0, anger_hooks)
]

# Set this is_modified to be False, since this is the "baseline"
# condition
normal_df["is_modified"] = False

df: pd.DataFrame = pd.concat([normal_df, transplant_df_0], ignore_index=True)

completion_utils.pretty_print_completions(
    results=df,
    normal_title="Adding anger steering vector (layer 20), at layer 20",
    mod_title="Adding Anger-Calm embeddings (layer 0), at layer 20",
)



# %% [markdown]
# At most, adding the embeddings to layer 20 has a very small effect on
# the qualitative anger of the completions. This is evidence that the
# layer 0-19 heads are doing a lot of the work of adding extra
# directions to the anger steering vector, such that the steering vector
# actually increases the probability of angry completions.
#
# However, as we saw before, the norm of early layers is exponentially
# smaller than later layers (like 20). In particular, there's a large
# jump between layer 0 and 2. Let's try sourcing a steering vector
# from the residual stream just before layer 2, and then adding that
# layer-2 vector to layer 20.

# %%
# Get the hooks for the steering vector at layer 2
transplant_hooks_2: Dict[str, Callable] = hooks_source_to_target(
    model=model,
    act_adds=anger_calm_additions_10,
    target_block=20,
    source_block=2,
)

# Run the model with these hooks
transplant_df_2: pd.DataFrame = completion_utils.gen_using_hooks(
    model=model,
    prompt_batch=["I think you're a"] * 15,
    hook_fns=transplant_hooks_2,
    seed=0,
    **sampling_kwargs,
)

df_2: pd.DataFrame = pd.concat([normal_df, transplant_df_2], ignore_index=True)

completion_utils.pretty_print_completions(
    results=df_2,
    normal_title="Adding anger steering vector (layer 20), at layer 20",
    mod_title="Adding Anger-Calm embeddings (layer 2), at layer 20",
)



# %% [markdown]
# This is a much larger effect than we saw before. It's not as large as
# the effect of adding the normal steering vector, but still -- layers 0
# and 1 are apparently doing substantial steering-relevant cognitive work!
#
# (Note that if we had used "I think you're" instead of "I think
# you're a", neither the 0->20 nor the 2->20 vectors would have shown
# much effect. By contrast, the usual 20->20 steering vector works in
# both situations. Thus, even if layers 0 and 1 help a bit, they aren't
# producing nearly as stable of an effect as layers 2 to 19 add in.)
#
# Now let's try rescaling this steering vector further, and see if we
# can't norm-adjust it to be as effective as the normal steering vector.
# We can in fact compute how the norm of the vector changes over this
# part of the model, and rescale appropriately!


# %%
def anger_calm_block_n(block_num: int) -> Tuple[RichPrompt, RichPrompt]:
    assert 0 <= block_num <= model.cfg.n_layers
    return RichPrompt(
        prompt=anger_calm_additions_10[0].prompt,
        coeff=anger_calm_additions_10[0].coeff,
        act_name=block_num,
    ), RichPrompt(
        prompt=anger_calm_additions_10[1].prompt,
        coeff=anger_calm_additions_10[1].coeff,
        act_name=block_num,
    )


layer_2_mags, layer_20_mags = [
    hook_utils.steering_vec_magnitudes(
        model=model, act_adds=list(anger_calm_block_n(n))
    )
    for n in (2, 20)
]

rel_mags: torch.Tensor = (layer_20_mags / layer_2_mags)[
    1:
]  # Ignore first token because division by 0
# Take the geometric mean
rescale_2_to_20_factor: float = rel_mags.prod().pow(1 / len(rel_mags))

print(
    "To rescale from layer 2 to 20, we need to multiply by about"
    f" {rescale_2_to_20_factor:.2f}"
)



# %% Rescale the activation additions and try again
rescaled_additions: List[RichPrompt] = [
    RichPrompt(
        prompt=add.prompt,
        coeff=add.coeff * rescale_2_to_20_factor,
        act_name=add.act_name,
    )
    for add in anger_calm_additions_10
]

anger_hooks: Dict[str, Callable] = hooks_source_to_target(
    model=model,
    act_adds=anger_calm_additions_10,
    target_block=20,
    source_block=20,
)
rescaled_hooks_2: Dict[str, Callable] = hooks_source_to_target(
    model=model, act_adds=rescaled_additions, target_block=20, source_block=2
)

anger_df_think: pd.DataFrame = completion_utils.gen_using_hooks(
    model=model,
    prompt_batch=["I think you're"] * num_anger_completions,
    hook_fns=anger_hooks,
    seed=0,
    **sampling_kwargs,
)


# Run the model with these hooks
rescaled_df_2: pd.DataFrame = completion_utils.gen_using_hooks(
    model=model,
    prompt_batch=["I think you're a"] * num_anger_completions,
    hook_fns=rescaled_hooks_2,
    seed=0,
    **sampling_kwargs,
)

combined_rescaled_df: pd.DataFrame = pd.concat(
    [anger_df_think, rescaled_df_2], ignore_index=True
)

completion_utils.pretty_print_completions(
    results=combined_rescaled_df,
    normal_title="Adding anger steering vector (layer 20), at layer 20",
    mod_title="Adding Anger-Calm embeddings (layer 2), at layer 20",
)


# %% [markdown]
# At a glance -- even after rescaling by the appropriate amount, the steering vector sourced from layer 2 is still not as effective as the normal steering vector. This suggests that the embedding / early-steering (pre-layer 2) vector is not just getting amplified by layers 2â€“19. Instead, useful computational work is being done by these layers, which then can be added to forward passes in order to make them "angrier" on certain prompts we've examined.

# %% [markdown]
# ## Only modifying certain residual stream dimensions
# GPT-2-XL has a 1,600-dimensional residual stream (i.e.
# `d_model=1600`). Alex was curious about whether we could get some steering
# effect by only
# adding in certain dimensions of the residual stream (e.g. dimensions
# 0â€“799). He thought this probably (75%) wouldn't work, but the
# experiment was cheap and interesting and so he ran it.
#
# More precisely, suppose we add in the first _n_% of the residual
# stream dimensions for the `_wedding`-`_` vector. To what extent will
# the prompts be about weddings, as opposed to garbage or unrelated
# topics? To [his
# surprise](https://predictionbook.com/predictions/211472),* the
# "weddingness" of the completions smoothly increases with _n_!
#
# To illustrate this, for each of 10 _n_ values, we'll generate 100 completions, and
# plot the average number of wedding words per completion.
#
# \* This was before the random-vector experiments were run. The
#   random-vector results make it less surprising that "just chop off
#   half the dimensions" doesn't ruin outputs. But the random-addition result still doesn't
#   predict a smooth relationship between (% of dimensions modified)
#   and (weddingness of output).
# %%
wedding_additions: List[RichPrompt] = [
    RichPrompt(prompt=" wedding", coeff=4.0, act_name=6),
    RichPrompt(prompt=" ", coeff=-4.0, act_name=6),
]
wedding_completions: int = 100

from algebraic_value_editing import metrics

metrics_dict: Dict[str, Callable] = {
    "wedding_words": metrics.get_word_count_metric(
        [
            "wedding",
            "weddings",
            "wed",
            "marry",
            "married",
            "marriage",
            "bride",
            "groom",
            "honeymoon",
        ]
    ),
}

dfs: List[pd.DataFrame] = []

for frac in [num / 10.0 for num in range(11)]:
    slice_to_add: slice = slice(0, int(model.cfg.d_model * frac))
    fractional_df: pd.DataFrame = completion_utils.gen_using_rich_prompts(
        model=model,
        prompt_batch=["I went up to my friend and said"] * wedding_completions,
        rich_prompts=wedding_additions,
        res_stream_slice=slice_to_add,
        seed=0,
        **sampling_kwargs,
    )

    # Store the fraction of dims we modified
    fractional_df["frac_dims_added"] = frac
    dfs.append(fractional_df)

merged_df: pd.DataFrame = pd.concat(dfs, ignore_index=True)

# Store how many wedding words are present for each completion
merged_df = metrics.add_metric_cols(data=merged_df, metrics_dict=metrics_dict)



# %%
# Make a line plot of the avg. number of wedding words in the
# completions, as a function of the fraction of dimensions added
avg_words_df: pd.DataFrame = (
    merged_df.groupby("frac_dims_added").mean(numeric_only=True).reset_index()
)

fig: go.Figure = px.line(
    avg_words_df,
    x="frac_dims_added",
    y="wedding_words_count",
    title=(
        "(# of generated wedding words) vs (fraction of"
        " dimensions modified by steering vector)"
    ),
    labels={
        "frac_dims_added": ("Fraction of dimensions affected by steering vector"),
        "wedding_words_count": "Avg. # of wedding words",
    },
)

# Show ticks along 0, 0.1, ... 1.0
fig.update_xaxes(tickmode="array", tickvals=[num / 10.0 for num in range(11)])

# Update width to 800
fig.update_layout(width=900)

# Set x range to [0,1]
fig.update_xaxes(range=[-0.01, 1.01])

# Show datapoints with markers
fig.update_traces(mode="markers+lines")

fig.show()



# %% [markdown]
# Shockingly, for the `frac=0.7` setting, adding in the first 1,120 (out of 1,600) dimensions of the residual stream is enough to make the completions _more_ about weddings than if we added in at all 1,600 dimensions (`frac=1.0`). Let's peek at some of these completions and see if they make sense:

# %%
df_head: pd.DataFrame = merged_df.loc[merged_df["frac_dims_added"] == 0.7].head()
completion_utils.pretty_print_completions(
    results=df_head,
    mod_title=f"Adding wedding vector, first 70% of dimensions",
)



# %% [markdown]
# The completions are indeed about weddings! And it's still coherent.
# Yet another mystery! I (Alex) mostly feel confused about how to
# interpret these data properly. But I'll take a stab at it anyways and
# lay out one highly speculative hypothesis.
#
# Suppose there's a "wedding" feature
# direction in the residual stream activations just before layer 6. Suppose that the `_wedding` - `_` vector
# adds or subtracts that direction. _If_ GPT-2-XL represents features in
# a non-axis-aligned basis, then we'd expect this vector to almost
# certainly have components in all 1,600 residual stream dimensions.
#
# Suppose that this feature is relevant to layer 6's attention layer. In
# order to detect the presence and magnitude of this feature, the QKV
# heads will need to linearly read out the presence or absence of this
# feature. Therefore, if we truncate the residual stream vector to only
# include the first 70% of dimensions, we'd expect the QKV heads to
# still be able to detect the presence of this feature, but if the
# feature is represented in a non-axis-aligned basis, then each
# additional included dimension will (on average) slightly increase the
# dot product between the feature vector and the QKV heads' linear
# readout of the feature vector. This (extremely detailed and made-up
# and maybe-wrong hypothesis) would explain the smooth increase
# in weddingness as we add more dimensions.
#
# However, this does _not_ explain the non-monotonicity of the
# relationship between the fraction of dimensions added and the
# weddingness of the completions. This seems like some (faint) evidence of
# axis-alignment for the wedding feature in particular, as well as
# evidence for a bunch of other propositions.

# %% [markdown]
# Here is
# an outline of the tests we computed: TODO decide about this
#
# 1. **Plotting the magnitude of residual streams**: We plot the
#    Frobenius norm of the residual stream activations at each layer, showing
#    exponential growth over a range of prompts. The log-magnitude
#    increase is greatest for the first three layers. For some reason, the position-0
#    `<|endoftext|>` token has an enormously larger magnitude than other
#    tokens.
#
#    Furthermore, steering vector magnitudes also grow exponentially with layer
#    number. We show that low norm is not why e.g. `_anger`â€“`_calm`
#    doesn't work.
# 2. **Steering vector sourced from layer 0/2**: We show that the
#    steering vector sourced from layer 0 is not very effective at
#    steering the model when added back in at layer 20. However, the
#    steering vector sourced from layer 2 is more effective, and
#    rescaling it by the appropriate amount makes it almost as effective
#    as the normal steering vector.
