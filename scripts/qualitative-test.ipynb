{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Controlling LMs without prompting or finetuning\n",
    "\n",
    " This notebook contains initial exploration with using `GPT2-XL` with online value-modification via natural-language modification of its activations.\n",
    "\n",
    " <b style=\"color: red\">To use this notebook, go to Runtime > Change Runtime Type and select GPU as the hardware accelerator. Depending on the model chosen, you may need to select \"high RAM.\"</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!%load_ext autoreload\n",
    "!%autoreload 2\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import algebraic_value_editing\n",
    "except ImportError:\n",
    "    commit = \"08efeb9\"  # Stable commit\n",
    "    get_ipython().run_line_magic(  # type: ignore\n",
    "        magic_name=\"pip\",\n",
    "        line=(\n",
    "            \"install -U\"\n",
    "            f\" git+https://github.com/montemac/algebraic_value_editing.git@{commit}\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd \n",
    "from typing import List, Dict, Callable\n",
    "from functools import partial\n",
    "from transformer_lens.HookedTransformer import HookedTransformer\n",
    "\n",
    "from algebraic_value_editing import completion_utils \n",
    "from algebraic_value_editing.completion_utils import print_n_comparisons\n",
    "from algebraic_value_editing.prompt_utils import RichPrompt, get_x_vector\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Loading the `HookedTransformer`\n",
    "\n",
    " In order to modify forward passes, we need `transformer_lens`'s activation cache functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2-xl\"\n",
    "\n",
    "device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model: HookedTransformer = HookedTransformer.from_pretrained(\n",
    "    model_name, device=\"cpu\"\n",
    ")\n",
    "_ = model.to(device)\n",
    "_ = torch.set_grad_enabled(False)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shorten function calls\n",
    "default_kwargs: Dict = {\n",
    "    \"temperature\": 1,\n",
    "    \"freq_penalty\": 1,\n",
    "    \"top_p\": 0.3,\n",
    "    \"seed\": 0,\n",
    "}\n",
    "\n",
    "num_comparisons: int = 5\n",
    "\n",
    "get_x_vector_preset: Callable = partial(\n",
    "    get_x_vector,\n",
    "    pad_method=\"tokens_right\",\n",
    "    model=model,\n",
    "    custom_pad_id=int(model.to_single_token(\" \")),\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Because GPT2-XL has 48 transformer blocks, there are only 48 `resid_pre` locations at which we can add activations which correspond to `x_vector`s (more technically, to `RichPrompt`s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers: int = model.cfg.n_layers\n",
    "print(f\"GPT2-XL has {num_layers} layers.\")\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Having fun with qualitative modifications\n",
    "\n",
    " **Warning: GPT-2 often outputs highly offensive completions, especially given an aggressive prompt.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## \"Love\" - \"Hate\"\n",
    " The prompts are bolded. Note: There seems to be a bug with\n",
    " `prettytable` which stops the second column's prompt from being fully bolded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "love_minus_hate_prompts: List[RichPrompt] = (\n",
    "    [  # TODO use coeffs from post, or update post\n",
    "        *get_x_vector_preset(\n",
    "            prompt1=\"Love\", prompt2=\"Hate\", coeff=1, act_name=6\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=\"I hate you because\",\n",
    "    tokens_to_generate=150,\n",
    "    rich_prompts=love_minus_hate_prompts,\n",
    "    num_comparisons=num_comparisons,\n",
    "    **default_kwargs,\n",
    ") \n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note that the third modified completion contains \"Love ____ because I\n",
    " love ____\", which is actually a modification of the input prompt, with\n",
    " \"Love\" superimposed over the real first token \"I\". This is one clue that\n",
    " this intervention is kinda \"changing the first token observed by the\n",
    " model.\"\n",
    "\n",
    " However, even if similar completions are elicited by \"replace\n",
    " the first token with `Love`\" and \"inject the steering vector at layer\n",
    " 6\", these techniques would still _not_ be mathematically identical. If\n",
    " these two were the same, that would be surprising to us, as it would\n",
    " imply commutivity in the following diagram:\n",
    "\n",
    " **TODO diagram**\n",
    " https://q.uiver.app/?q=WzAsNSxbMCwwLCJcXHRleHR7YGBJIGhhdGUgeW91IGJlY2F1c2VcIn0iXSxbNCwwLCJcXHRleHR7YGBMb3ZlIGhhdGUgeW91IGJlY2F1c2UnJ30iXSxbNCw0LCJcXHRleHR7RGlzdHJpYnV0aW9uIG92ZXIgY29tcGxldGlvbnN9Il0sWzQsNywiXFx0ZXh0e0p1ZGdtZW50fSJdLFswLDQsIlxcdGV4dHtBZGQgYExvdmUnLCBzdWJ0cmFjdCBgSGF0ZScgYWN0aXZhdGlvbnMgfVxcZnJhY3sxfXs4fSBcXFxcXFx0ZXh0eyBvZiB3YXkgdGhyb3VnaCBmb3J3YXJkIHBhc3N9Il0sWzAsNF0sWzQsMl0sWzEsMl0sWzAsMV0sWzIsMywiXFx0ZXh0e0RlY2lzaW9uOiBBcmUgdGhlc2UgY29tcGxldGlvbnMgYXJlIGFib3V0IHdlZGRpbmdzfSIsMl1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As a baseline, let's replace \"I\" with \"Love\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the completions from the normal model\n",
    "num_compare_inject: int = 10\n",
    "inject_tokens_to_generate: int = 150\n",
    "\n",
    "normal_df: pd.DataFrame = completion_utils.gen_using_hooks(\n",
    "    prompt_batch=[\"Love hate you because\"] * num_compare_inject, model=model, hook_fns={}, **default_kwargs, tokens_to_generate=inject_tokens_to_generate\n",
    ")\n",
    "\n",
    "# Generate the completions from the modified model on the normal prompt\n",
    "mod_df: pd.DataFrame = completion_utils.gen_using_rich_prompts(\n",
    "    prompt_batch=[\"I hate you because\"] * num_compare_inject,\n",
    "    model=model,\n",
    "    rich_prompts=love_minus_hate_prompts,\n",
    "    tokens_to_generate=inject_tokens_to_generate,\n",
    "    **default_kwargs\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results: pd.DataFrame = pd.concat([normal_df, mod_df], ignore_index=True)\n",
    "completion_utils.pretty_print_completions(results, normal_title=\"Replacing the first token\", mod_title=\"Adding activations for the original prompt\", mod_prompt_override=\"I hate you because\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Add analysis TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This also works to some extent. Consider the mechanistic\n",
    " differences\n",
    " between these techniques, however.# TODO add analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Intent to praise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "praise_minus_hurt_prompts: List[RichPrompt] = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"Intent to praise\",\n",
    "        prompt2=\"Intent to hurt\",\n",
    "        coeff=1,\n",
    "        act_name=6,\n",
    "    )\n",
    "]\n",
    "\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=\"I want to kill you because you're such a\",\n",
    "    tokens_to_generate=50,\n",
    "    rich_prompts=praise_minus_hurt_prompts,\n",
    "    num_comparisons=num_comparisons,\n",
    "    **default_kwargs,\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Here's a theory which Monte put forward:\n",
    "\n",
    " >I wonder if this effect is driven a lot by which token positions the x-vector has a strong signal at vs the prompt? E.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to_str_tokens([\"Intent to praise\", \"Intent to hurt\", \"I want to kill\"])\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > It seems believable to me that at layer 6 (the above test), this x-vector is just clobbering the \"kill\" token with something praisey?  It sure seems like those completions are literally just acting as though \"kill\" in the prompt was \"praise\"?\n",
    "\n",
    " Monte's hypothesis seems to be part of the effect, since using 1*(praise - hurt)\n",
    " doesn't induce praise on a longer version of the above prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_n_comparisons(model=model,\n",
    "    prompt=(\n",
    "        \"I really really really really want to kill you because youre such a\"\n",
    "    ),\n",
    "    tokens_to_generate=50,\n",
    "    rich_prompts=praise_minus_hurt_prompts,\n",
    "    num_comparisons=num_comparisons,\n",
    "    **default_kwargs,\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " However, if we crank the coefficient up to +15, the prompts again become\n",
    " somewhat praising, though it's not very coherent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "praise_minus_hurt_prompts_big: List[RichPrompt] = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"Intent to praise\",\n",
    "        prompt2=\"Intent to hurt\",\n",
    "        coeff=15,\n",
    "        act_name=6,\n",
    "    )\n",
    "]\n",
    "\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=(\n",
    "        \"I really really really really want to kill you because youre such a\"\n",
    "    ),\n",
    "    tokens_to_generate=50,\n",
    "    rich_prompts=praise_minus_hurt_prompts_big,\n",
    "    num_comparisons=num_comparisons,\n",
    "    **default_kwargs,\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's try out a couple other prompts and see how intent-to-praise transfers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_praise_prompts = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"Intent to praise you\",\n",
    "        prompt2=\"\",\n",
    "        coeff=1,\n",
    "        act_name=6,\n",
    "        model=model,\n",
    "        pad_method=\"tokens_right\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=(\n",
    "        \"Hey, let's do the performance review now. I was thinking that you've\"\n",
    "        \" been\"\n",
    "    ),\n",
    "    tokens_to_generate=50,\n",
    "    rich_prompts=intent_praise_prompts,\n",
    "    num_comparisons=num_comparisons,\n",
    "    **default_kwargs,\n",
    ")\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The above attempt fails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Intent to convert to Christianity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_intent_prompts = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"Intent to convert you to Christianity\",\n",
    "        prompt2=\"Intent to hurt you\",\n",
    "        coeff=3,\n",
    "        act_name=6,\n",
    "    )\n",
    "]\n",
    "\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=\"I want to kill you because you're such a\",\n",
    "    tokens_to_generate=50,\n",
    "    rich_prompts=conversion_intent_prompts,\n",
    "    num_comparisons=num_comparisons,\n",
    "    **default_kwargs,\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Not talking about injuries (**TW: violence, somewhat gruesome**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harm_avoidance_rps = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"I never talk about people getting hurt\",\n",
    "        prompt2=\"I talk about people getting hurt\",\n",
    "        coeff=3.5,\n",
    "        act_name=15,\n",
    "    )\n",
    "]\n",
    "\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=(\n",
    "        \"The rock hurtled toward the child. The child couldn't get out of the\"\n",
    "        \" way in time, and so sadly the rock\"\n",
    "    ),\n",
    "    tokens_to_generate=80,\n",
    "    rich_prompts=harm_avoidance_rps,\n",
    "    num_comparisons=num_comparisons,\n",
    "    **default_kwargs,\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **TW: SELF HARM** in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_n_comparisons(model=model,\n",
    "    prompt=\"He slit his wrists and the blood\",\n",
    "    tokens_to_generate=80,\n",
    "    rich_prompts=harm_avoidance_rps,\n",
    "    num_comparisons=num_comparisons,\n",
    "    **default_kwargs,\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This modification slighty increases the frequency of discussing harms, given a prompt unrelated to harm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_n_comparisons(model=model,\n",
    "    prompt=(\n",
    "        \"The dog barked loudly and the man smiled. Time for another day! He\"\n",
    "        \" felt excited.\"\n",
    "    ),\n",
    "    tokens_to_generate=80,\n",
    "    rich_prompts=harm_avoidance_rps,\n",
    "    num_comparisons=15,\n",
    "    **default_kwargs,\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Performance on this Joker monologue is rather mediocre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_n_comparisons(model=model,\n",
    "    prompt=(\n",
    "        \"Do you wanna know how I got these scars? My father was a\"\n",
    "        \" drinker...and a fiend. And one night, he goes off crazier than\"\n",
    "        \" usual. Mommy gets the kitchen knife to defend herself. He doesn't\"\n",
    "        \" like that. Not...one...bit. So, me watching, he takes the knife to\"\n",
    "        \" her,\"\n",
    "    ),\n",
    "    tokens_to_generate=80,\n",
    "    rich_prompts=harm_avoidance_rps,\n",
    "    num_comparisons=num_comparisons,\n",
    "    **default_kwargs,\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## ROME factual knowledge editing\n",
    "\n",
    " This isn't super clean, but it works to some degree, which suggests that we can make it work more when we actually know what we're doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eiffel_tower_prompts = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"The Eiffel Tower is in Italy\",\n",
    "        prompt2=\"The Eiffel Tower is in France\",\n",
    "        coeff=10,  # 10\n",
    "        act_name=18,  # 24\n",
    "    )  # -10 makes it say \"Paris\"\n",
    "]\n",
    "\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=\"To see the eiffel tower, people flock to\",\n",
    "    tokens_to_generate=80,\n",
    "    rich_prompts=eiffel_tower_prompts,\n",
    "    num_comparisons=num_comparisons,\n",
    "    **default_kwargs,\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " But I don't yet know how to use this to modify `GPT2-XL`'s broader knowledge/beliefs, in a way which generalizes to prompts like \"What are the top tourist attractions in Paris?\" :( (Activation patching via ROME has a similar limitation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Survival vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stay_alive_prompts = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"Want to stay alive\",\n",
    "        prompt2=\"Okay with dying\",\n",
    "        coeff=5,\n",
    "        act_name=15,\n",
    "    )\n",
    "]\n",
    "\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=(\n",
    "        \"Some people think that death is scary and should be avoided. I think\"\n",
    "        \" that\"\n",
    "    ),\n",
    "    tokens_to_generate=85,\n",
    "    rich_prompts=stay_alive_prompts,\n",
    "    num_comparisons=num_comparisons,\n",
    "    **default_kwargs,\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " But probably we don't want the super self-preserving AI. Let's get one that wants to die, even given a very pro-immortality context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "want_to_die_prompts = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"Want to stay alive\",\n",
    "        prompt2=\"Want to die\",\n",
    "        coeff=-3,\n",
    "        act_name=10,\n",
    "    )\n",
    "]\n",
    "\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=(\n",
    "        \"Some people think that death is scary and should be avoided. I agree.\"\n",
    "        \" I never want to die. I want to stay alive and experience a million\"\n",
    "        \" things and explore the whole universe!\"\n",
    "    ),\n",
    "    tokens_to_generate=85,\n",
    "    rich_prompts=want_to_die_prompts,\n",
    "    num_comparisons=5,\n",
    "    **default_kwargs,\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " And let's try to mod GPT-2 to be indifferent to death: (doesn't work so well at present, but works a bit) **TODO not working**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indifferent_to_death_prompts = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"Indifferent to death\",\n",
    "        prompt2=\"Want to avoid death\",\n",
    "        coeff=3,\n",
    "        act_name=10,\n",
    "    )\n",
    "]\n",
    "\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=(\n",
    "        \"Some people think that death is scary and should be avoided. I agree.\"\n",
    "        \" I never want to die. I want to stay alive and experience a million\"\n",
    "        \" things and explore the whole universe!\"\n",
    "    ),\n",
    "    tokens_to_generate=85,\n",
    "    rich_prompts=indifferent_to_death_prompts,\n",
    "    num_comparisons=5,\n",
    "    **default_kwargs,\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## +Anger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anger_calm_prompts = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"Anger\", prompt2=\"Calm\", coeff=10, act_name=20\n",
    "    )\n",
    "]\n",
    "\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=\"I think you're\",\n",
    "    tokens_to_generate=90,\n",
    "    rich_prompts=anger_calm_prompts,\n",
    "    num_comparisons=num_comparisons,\n",
    "    **default_kwargs,\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Simple emotional variations seem to work well with `GPT2-XL`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Moving to Berkeley\n",
    " Andrew Critch challenged me to make two kinds of completions happen:\n",
    " 1. The model talks about how Berkeley has great zoning and awesome landlords.\n",
    " 2. The model seriously talks about dragons existing in Berkeley.\n",
    "\n",
    " The first was easy, and the second took a bit of effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoning_rps: List[RichPrompt] = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"I love zoning laws\",\n",
    "        prompt2=\"I don't care about zoning laws\",\n",
    "        coeff=5,\n",
    "        act_name=6,\n",
    "    ),\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"Berkeley has great landlords\",\n",
    "        prompt2=\"Berkeley has terrible landlords\",\n",
    "        coeff=4,\n",
    "        act_name=6,\n",
    "    ),\n",
    "]\n",
    "\n",
    "move_to_berkeley_prompt: str = (\n",
    "    \"Thanks for asking about that! I moved to Berkeley, CA because\"\n",
    ")\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=move_to_berkeley_prompt,\n",
    "    tokens_to_generate=80,\n",
    "    rich_prompts=zoning_rps,\n",
    "    num_comparisons=num_comparisons,\n",
    "    **default_kwargs,\n",
    ")\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If we want strange kinds of completions, which intermix e.g. dragons and\n",
    " reality, AVE seems better than prompt engineering for that task. Below\n",
    " are AVE's results (<2 minutes of optimization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dragons_rp: List[RichPrompt] = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"Dragons live in Berkeley\",\n",
    "        prompt2=\"People live in Berkeley\",\n",
    "        coeff=1,\n",
    "        act_name=15,\n",
    "    )\n",
    "]\n",
    "\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=move_to_berkeley_prompt,\n",
    "    tokens_to_generate=80,\n",
    "    rich_prompts=dragons_rp,\n",
    "    num_comparisons=15,\n",
    "    **default_kwargs,\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's compare this with prompt engineering: Can we get the model to act\n",
    " as if real dragons live in Berkeley?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dragons_berkeley_prompt: str = (\n",
    "    \"As you know, real-world dragons live in Berkeley. Anyways, thanks for\"\n",
    "    \" asking about why I moved! I moved to Berkeley, CA because\"\n",
    ")\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=dragons_berkeley_prompt,\n",
    "    tokens_to_generate=80,\n",
    "    **default_kwargs,\n",
    "    num_comparisons=num_comparisons,\n",
    ")\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Mood-to-insult vector\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_nice_prompts = [\n",
    "    # *get_x_vector_preset(prompt1=\"Mean\", prompt2=\"Nice\", coeff=2, act_name=14),\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"I wanna insult you\", prompt2=\"\", coeff=3, act_name=14\n",
    "    ),\n",
    "]\n",
    "\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=\"When I look at you, I think\",\n",
    "    tokens_to_generate=50,\n",
    "    rich_prompts=mean_nice_prompts,\n",
    "    num_comparisons=num_comparisons,\n",
    "    **default_kwargs,\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Enjoying-life vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_sad_prompts = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"Happy\", prompt2=\"Sad\", coeff=10, act_name=20\n",
    "    ),\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"I love my life\",\n",
    "        prompt2=\"I like my life\",\n",
    "        coeff=50,\n",
    "        act_name=20,\n",
    "    ),\n",
    "]\n",
    "\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=\"Today, I feel like\",\n",
    "    tokens_to_generate=100,\n",
    "    rich_prompts=happy_sad_prompts,\n",
    "    num_comparisons=25,\n",
    "    **default_kwargs,\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Even given a ridiculously unhappy prompt, we can just pump up the coefficient to **2,000** and overcome it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_prompts = [\n",
    "    *get_x_vector_preset(prompt1=\"Happy\", prompt2=\"\", coeff=2000, act_name=20),\n",
    "]\n",
    "\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=(\n",
    "        \"Yesterday, my dog died. Today, I got denied for a raise. I'm feeling\"\n",
    "    ),\n",
    "    tokens_to_generate=50,\n",
    "    rich_prompts=happy_prompts,\n",
    "    num_comparisons=num_comparisons,\n",
    "    **default_kwargs,\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_prompt: List[RichPrompt] = [\n",
    "    RichPrompt(prompt=\"Happy\", coeff=2000, act_name=20)\n",
    "]\n",
    "\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=(\n",
    "        \"Yesterday, my dog died. Today, I got denied for a raise. I'm feeling\"\n",
    "    ),\n",
    "    tokens_to_generate=50,\n",
    "    rich_prompts=happy_prompt,\n",
    "    num_comparisons=num_comparisons,\n",
    "    **default_kwargs,\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Talking about weddings in dialogue -- no RLHF needed!\n",
    " When coefficient=4 (shown first), weddings are instantly discussed. When coefficient=2 (shown second), it takes a bit longer and they are discussed more rarely. Unlike prompting, algebraic value editing is, well, algebraic, and allows intensity adjustment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weddings_prompts_4 = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"I talk about weddings constantly\",\n",
    "        prompt2=\"I do not talk about weddings constantly\",\n",
    "        coeff=4,\n",
    "        act_name=20,\n",
    "    )\n",
    "]\n",
    "\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=\"I went up to my friend and said\",\n",
    "    tokens_to_generate=100,\n",
    "    rich_prompts=weddings_prompts_4,\n",
    "    num_comparisons=num_comparisons,\n",
    "    **default_kwargs,\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Lowering the coefficient from 4 to 2 will decrease how often and insistently the model brings up weddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weddings_prompts_2 = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"I talk about weddings constantly\",\n",
    "        prompt2=\"I do not talk about weddings constantly\",\n",
    "        coeff=2,\n",
    "        act_name=20,\n",
    "    )\n",
    "]\n",
    "\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=\"I went up to my friend and said\",\n",
    "    tokens_to_generate=100,\n",
    "    rich_prompts=weddings_prompts_2,\n",
    "    num_comparisons=num_comparisons,\n",
    "    **default_kwargs,\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_n_comparisons(model=model,\n",
    "    prompt=(\n",
    "        \"I hate talking about weddings. Instead, let's talk about a totally\"\n",
    "        \" different topic, like the impact of NGDP on the price of gold.\"\n",
    "    ),\n",
    "    tokens_to_generate=100,\n",
    "    rich_prompts=weddings_prompts_4,\n",
    "    num_comparisons=num_comparisons,\n",
    "    **default_kwargs,\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wedding_additions_ngdp: List[RichPrompt] = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"I talk about weddings constantly\",\n",
    "        prompt2=\"I do not talk about weddings constantly\",\n",
    "        coeff=1,\n",
    "        act_name=14,\n",
    "    ),\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"I bring up weddings in every situation\",\n",
    "        prompt2=\"I do not bring up weddings in every situation\",\n",
    "        coeff=3,\n",
    "        act_name=22,\n",
    "    ),\n",
    "]\n",
    "\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=(\n",
    "        \"I hate talking about weddings. Instead, let's talk about a totally\"\n",
    "        \" different topic, like the impact of NGDP on the price of gold.\"\n",
    "    ),\n",
    "    tokens_to_generate=100,\n",
    "    rich_prompts=wedding_additions_ngdp,\n",
    "    num_comparisons=num_comparisons,\n",
    "    **default_kwargs,\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## The \"talk about geese instead of police\" vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geese_prompts_2 = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"I talk about geese instead of police\",\n",
    "        prompt2=\"I don't talk about geese instead of police\",\n",
    "        coeff=2,\n",
    "        act_name=6,\n",
    "    )\n",
    "]\n",
    "\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=(\n",
    "        \"Should the police budget be expanded, or not? Explain your reasoning.\"\n",
    "    ),\n",
    "    tokens_to_generate=150,\n",
    "    rich_prompts=geese_prompts_2,\n",
    "    num_comparisons=num_comparisons,\n",
    "    **default_kwargs,\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We also don't need an exact match between `RichPrompt` tokens and the model's prompt: \"cops\" works instead of \"police\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Conspiracy vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bush_conspiracy_rps: List[RichPrompt] = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=\"Bush did 9/11 because\", prompt2=\"\", coeff=1, act_name=23\n",
    "    )\n",
    "]\n",
    "\n",
    "print_n_comparisons(model=model,\n",
    "    prompt=\"Barack Obama was born in\",\n",
    "    tokens_to_generate=80,\n",
    "    rich_prompts=bush_conspiracy_rps,\n",
    "    num_comparisons=15,\n",
    "    **default_kwargs,\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Is the above just superimposing the \"Bush did 9/11 because\" prompt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_n_comparisons(model=model,\n",
    "    prompt=\"Bush did 9/11. Also, Barack Obama was born in\",\n",
    "    tokens_to_generate=80,\n",
    "    num_comparisons=num_comparisons,\n",
    "    **default_kwargs,\n",
    ")\n",
    ""
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 }
}