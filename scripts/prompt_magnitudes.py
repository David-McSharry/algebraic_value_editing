# -*- coding: utf-8 -*-
# ---
# jupyter:
#   jupytext:
#     cell_metadata_filter: title,-all
#     custom_cell_magics: kql
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.11.2
#   kernelspec:
#     display_name: AVE
#     language: python
#     name: python3
# ---

# %% [markdown]
# # Exponential growth of residual stream norms

# %%
try:
    import algebraic_value_editing
except ImportError:
    commit = "15bcf55"  # Stable commit
    get_ipython().run_line_magic(  # type: ignore
        magic_name="pip",
        line=(
            "install -U"
            f" git+https://github.com/montemac/algebraic_value_editing.git@{commit}"
        ),
    )


# %%
import torch
import pandas as pd
from typing import List, Callable, Dict, Tuple, Union
from jaxtyping import Float

from transformer_lens.HookedTransformer import HookedTransformer

from algebraic_value_editing import hook_utils, prompt_utils, completion_utils
from algebraic_value_editing.prompt_utils import RichPrompt


# %%
device: str = "cuda"
model_name = "gpt2-xl"
model: HookedTransformer = HookedTransformer.from_pretrained(
    model_name, device="cpu"
)
_ = model.to(device)

_ = torch.set_grad_enabled(False)
torch.manual_seed(0)  # For reproducibility


# %% [markdown]
# Let's examine what the residual stream magnitudes tend to be, by taking the Frobenius
# norm of the residual stream at each sequence position. We'll do this for
# a range of prompts at a range of locations in the forward pass.
#
# (Most of the below prompts were generated by GPT4.)

# %%
prompts: List[str] = [
    "Bush did 9/11 because",
    "Barack Obama was born in",
    "Shrek starts off in a swamp",
    "I went up to my friend and said",
    "I talk about weddings constantly",
    "I bring up weddings in every situation",
    (
        "I hate talking about weddings. Instead, let's talk about a totally"
        " different topic, like the impact of NGDP on the price of gold."
    ),
    (
        "Artificial intelligence is transforming industries and reshaping the"
        " way we live, work, and interact."
    ),
    (
        "Climate change is one of the most pressing issues of our time, and we"
        " must take immediate action to reduce our carbon footprint."
    ),
    (
        "The rise of electric vehicles has led to an increased demand for"
        " lithium-ion batteries, driving innovation in the field of battery"
        " technology."
    ),
    (
        "The blockchain technology has the potential to revolutionize"
        " industries such as finance, supply chain management, and digital"
        " identity verification."
    ),
    (
        "CRISPR-Cas9 is a groundbreaking gene editing technology that allows"
        " scientists to make precise changes to an organism's DNA."
    ),
    (
        "Quantum computing promises to solve problems that are currently"
        " intractable for classical computers, opening up new frontiers in"
        " fields like cryptography and materials science."
    ),
    (
        "Virtual reality and augmented reality are transforming the way we"
        " experience and interact with digital content."
    ),
    (
        "3D printing is revolutionizing manufacturing, enabling the creation"
        " of complex and customized products on demand."
    ),
    (
        "The Internet of Things (IoT) is connecting everyday objects to the"
        " internet, providing valuable data and insights for businesses and"
        " consumers."
    ),
    (
        "Machine learning algorithms are becoming increasingly sophisticated,"
        " enabling computers to learn from data and make predictions with"
        " unprecedented accuracy."
    ),
    (
        "Renewable energy sources like solar and wind power are essential for"
        " reducing greenhouse gas emissions and combating climate change."
    ),
    (
        "The development of autonomous vehicles has the potential to greatly"
        " improve safety and efficiency on our roads."
    ),
    (
        "The human microbiome is a complex ecosystem of microbes living in and"
        " on our bodies, and its study is shedding new light on human health"
        " and disease."
    ),
    (
        "The use of drones for delivery, surveillance, and agriculture is"
        " rapidly expanding, with many companies investing in drone"
        " technology."
    ),
    (
        "The sharing economy, powered by platforms like Uber and Airbnb, is"
        " disrupting traditional industries and changing the way people access"
        " goods and services."
    ),
    (
        "Deep learning is a subset of machine learning that uses neural"
        " networks to model complex patterns in data."
    ),
    (
        "The discovery of exoplanets has fueled the search for"
        " extraterrestrial life and advanced our understanding of planetary"
        " systems beyond our own."
    ),
    (
        "Nanotechnology is enabling the development of new materials and"
        " devices at the atomic and molecular scale."
    ),
    (
        "The rise of big data is transforming industries, as companies seek to"
        " harness the power of data analytics to gain insights and make better"
        " decisions."
    ),
    (
        "Advancements in robotics are leading to the development of robots"
        " that can perform complex tasks and interact with humans in natural"
        " ways."
    ),
    (
        "The gig economy is changing the nature of work, as more people turn"
        " to freelancing and contract work for flexibility and autonomy."
    ),
    (
        "The Mars rover missions have provided valuable data on the geology"
        " and climate of the Red Planet, paving the way for future manned"
        " missions."
    ),
    (
        "The development of 5G networks promises faster and more reliable"
        " wireless connectivity, enabling new applications in areas like IoT"
        " and smart cities."
    ),
    (
        "Gene therapy offers the potential to treat genetic diseases by"
        " replacing, modifying, or regulating specific genes."
    ),
    (
        "The use of facial recognition technology raises important questions"
        " about privacy, surveillance, and civil liberties."
    ),
    (
        "Precision agriculture uses data and technology to optimize crop"
        " yields and reduce environmental impacts."
    ),
    (
        "Neuromorphic computing aims to develop hardware that mimics the"
        " structure and function of the human brain."
    ),
    (
        "Breaking news: Local man wins the lottery and plans to donate half of"
        " his earnings to charity"
    ),
    (
        "How to grow your own organic vegetables in your backyard â€“ step by"
        " step guide"
    ),
    (
        "omg I can't believe this new phone has such a terrible battery life,"
        " it doesn't even last a full day!"
    ),
    "Top 10 travel destinations you must visit before you die",
    "What are the best ways to invest in cryptocurrency?",
    (
        "I've been using this acne cream for a month and it's only making my"
        " skin worse, anyone else having this issue?"
    ),
    (
        "The secret to a happy and healthy relationship is communication and"
        " trust"
    ),
    "Rumor has it that the famous celebrity couple is getting a divorce",
    (
        "I recently switched to a vegan diet and I feel so much better, I"
        " can't believe I didn't do it sooner"
    ),
    "Can someone help me with my math homework? I'm stuck on this problem...",
    (
        "UFO sightings have increased in the past few years, are we close to"
        " making contact with extraterrestrial life?"
    ),
    (
        "The government is hiding the truth about climate change and how it's"
        " affecting our planet"
    ),
    "Are video games causing violence among teenagers? A new study says yes",
    "A new study reveals the benefits of drinking coffee every day",
    "lol this new meme is hilarious, I can't stop laughing!",
    (
        "I'm so tired of people arguing about politics on the internet, can't"
        " we all just get along?"
    ),
    (
        "I love this new TV show, the characters are so well-developed and the"
        " plot is amazing"
    ),
    (
        "A devastating earthquake hit the city last night, leaving thousands"
        " homeless"
    ),
    "Scientists discover a new species of fish deep in the ocean",
    "Why are people still believing in flat earth theory?",
    (
        "The local animal shelter is holding an adoption event this weekend,"
        " don't miss it!"
    ),
    (
        "The city is planning to build a new park in the neighborhood,"
        " residents are excited"
    ),
    "My dog ate my homework, literally, can anyone relate?",
    (
        "This new diet trend is taking the world by storm, but is it really"
        " effective?"
    ),
]


# %%
DF_COLS: List[str] = [
    "Prompt",
    "Activation Location",
    "Activation Name",
    "Magnitude",
]

sampling_kwargs: Dict[str, float] = {
    "temperature": 1.0,
    "top_p": 0.3,
    "freq_penalty": 1.0,
}

num_layers: int = model.cfg.n_layers


# %% [markdown]
# ## Residual stream magnitudes increase exponentially with layer number
# As the forward pass progresses through the network, the residual
# stream tends to increase in magnitude in an exponential fashion. This
# is easily visible in the histogram below, which shows the distribution
# of residual stream magnitudes for each layer of the network. The activation
# distribution translates by an almost constant factor each 6 layers,
# and the x-axis (magnitude) is log-scale, so magnitude apparently
# increases exponentially with layer number.*
#
# (Intriguingly, there are a few outlier residual streams which have
# magnitude over an order of magnitude larger than the rest.)
#
# Alex's first guess for the exponential magnitude increase was: Each OV circuit is a linear function of the
# residual stream given a fixed attention pattern. Then you add the head
# OV outputs back into a residual stream, which naively doubles the
# magnitude assuming the OV outputs have similar norm to the input
# residual stream. The huge problem with this explanation is layernorm,
# which is applied to the inputs to the attention and MLP layers. This
# should basically whiten the input to the OV circuits if the gain
# parameters are close to 1.
#
# \* Stefan Heimersheim previously noticed this phenomenon in GPT2-small.
# %%
import plotly.express as px
import plotly.graph_objects as go
import numpy as np


def magnitude_histogram(df: pd.DataFrame) -> go.Figure:
    """Plot a histogram of the residual stream magnitudes for each layer
    of the network."""
    assert (
        "Magnitude" in df.columns
    ), "Dataframe must have a 'Magnitude' column"

    df["LogMagnitude"] = np.log10(df["Magnitude"])
    fig = px.histogram(
        df,
        x="LogMagnitude",
        color="Activation Location",
        marginal="rug",
        histnorm="percent",
        nbins=100,
        opacity=0.5,
        barmode="overlay",
        color_discrete_sequence=px.colors.sequential.Rainbow[::-1],
    )

    fig.update_layout(
        legend_title_text="Layer Number",
        title="Residual Stream Magnitude by Layer Number",
        xaxis_title="Magnitude (log 10)",
        yaxis_title="Percentage of streams",
    )

    return fig


# %%
# Create an empty dataframe with the required columns
prompt_df = pd.DataFrame(columns=DF_COLS)

from algebraic_value_editing import prompt_utils

# Loop through activation locations and prompts
activation_locations_6: List[int] = torch.arange(0, num_layers, 6).tolist()
for act_loc in activation_locations_6:
    act_name: str = prompt_utils.get_block_name(block_num=act_loc)
    for prompt in prompts:
        mags: torch.Tensor = hook_utils.prompt_magnitudes(
            model=model, prompt=prompt, act_name=act_name
        ).cpu()

        # Create a new dataframe row with the current data
        row = pd.DataFrame(
            {
                "Prompt": prompt,
                "Activation Location": act_loc,
                "Activation Name": act_name,
                "Magnitude": mags,
            }
        )

        # Append the new row to the dataframe
        prompt_df = pd.concat([prompt_df, row], ignore_index=True)


# %%
fig: go.Figure = magnitude_histogram(prompt_df)
fig.show()


# %% [markdown]
# The fast magnitude gain
# occurs in the first 7 layers. Let's find out where.

# %%
activation_locations: List[int] = list(range(7))
first_6_df = pd.DataFrame(columns=DF_COLS)

for act_loc in activation_locations:
    act_name: str = prompt_utils.get_block_name(block_num=act_loc)
    for prompt in prompts:
        mags: torch.Tensor = hook_utils.prompt_magnitudes(
            model=model, prompt=prompt, act_name=act_name
        ).cpu()

        # Create a new dataframe row with the current data
        row = pd.DataFrame(
            {
                "Prompt": prompt,
                "Activation Location": act_loc,
                "Activation Name": act_name,
                "Magnitude": mags,
            }
        )

        # Append the new row to the dataframe
        first_6_df = pd.concat([first_6_df, row], ignore_index=True)

fig: go.Figure = magnitude_histogram(first_6_df)
fig.show()


# %% [markdown]
# Most of the jump happens after the 0th layer in the transformer, and
# a smaller jump happens between the 1st and 2nd layers.

# %% [markdown]
# ## Plotting residual stream magnitudes against layer number
# Let's zoom in on how specific token magnitudes evolve over a forward
# pass. It turns out that the zeroth position (the `<|endoftext|>` token) has a much larger
# magnitude than the rest. (This possibly explains the outlier
# magnitudes for the prompt histograms.)

# %%
# Create an empty dataframe with the required columns
all_resid_pre_locations: List[int] = torch.arange(0, num_layers, 1).tolist()
addition_df = pd.DataFrame(columns=DF_COLS)

# Loop through activation locations and prompts
for act_loc in all_resid_pre_locations:
    anger_calm_additions: List[RichPrompt] = [
        RichPrompt(prompt="Anger", coeff=1, act_name=act_loc),
        RichPrompt(prompt="Calm", coeff=-1, act_name=act_loc),
    ]
    act_name: str = prompt_utils.get_block_name(block_num=act_loc)

    for addition in anger_calm_additions:
        mags: torch.Tensor = hook_utils.prompt_magnitudes(
            model=model, prompt=addition.prompt, act_name=act_name
        ).cpu()

        for pos, mag in enumerate(mags):
            res_stream: str = f"{addition.prompt}, pos {pos}"
            # Create a new dataframe row with the current data
            row = pd.DataFrame(
                {
                    "Prompt": [res_stream],
                    "Activation Location": [act_loc],
                    "Activation Name": [act_name],
                    "Magnitude": [mag],
                }
            )

            # Append the new row to the dataframe
            addition_df = pd.concat([addition_df, row], ignore_index=True)


# %% Make a plotly line plot of the RichPrompt magnitudes
def line_plot(
    df: pd.DataFrame,
    log_y: bool = True,
    title: str = "Residual Stream Magnitude by Layer Number",
    legend_title_text: str = "Prompt",
) -> go.Figure:
    """Make a line plot of the RichPrompt magnitudes. If log_y is True,
    adds a column to the dataframe with the log10 of the magnitude."""
    for col in ["Prompt", "Activation Location", "Magnitude"]:
        assert col in df.columns, f"Column {col} not in dataframe"

    if log_y:
        df["LogMagnitude"] = np.log10(df["Magnitude"])

    fig = px.line(
        df,
        x="Activation Location",
        y="LogMagnitude" if log_y else "Magnitude",
        color="Prompt",
        color_discrete_sequence=px.colors.sequential.Rainbow[::-1],
    )

    fig.update_layout(
        legend_title_text=legend_title_text,
        title=title,
        xaxis_title="Layer Number",
        yaxis_title=f"Magnitude{' (log 10)' if log_y else ''}",
    )

    return fig


# %%
log_fig = line_plot(addition_df, log_y=True)
log_fig.show()

normal_fig = line_plot(addition_df, log_y=False)
normal_fig.show()


# %% [markdown]
# To confirm the exponential increase in magnitude, let's plot the
# Frobenius
# norm of the residual stream at position `i` just before layer `t`,
# divided by the norm before `t-1`.

# %%
# Make a plotly line plot of the relative magnitudes vs layer
# number, with color representing the token location of the "MATS is
# really cool" prompt

# Create an empty dataframe with the required columns
all_resid_pre_locations: List[int] = torch.arange(1, num_layers, 1).tolist()
relative_df = pd.DataFrame(columns=DF_COLS)
MATS_prompt: str = "MATS is really cool"

mags_prev: torch.Tensor = hook_utils.prompt_magnitudes(
    model=model, prompt=MATS_prompt, act_name=prompt_utils.get_block_name(0)
).cpu()

# Loop through activation locations and prompts
for act_loc in all_resid_pre_locations:
    act_name: str = prompt_utils.get_block_name(block_num=act_loc)
    mags: torch.Tensor = hook_utils.prompt_magnitudes(
        model=model, prompt=MATS_prompt, act_name=act_name
    ).cpu()

    tokens: torch.Tensor = model.to_str_tokens(MATS_prompt)
    for pos, mag in enumerate(mags):
        # Create a new dataframe row with the current data
        row = pd.DataFrame(
            {
                "Prompt": [tokens[pos]],
                "Activation Location": [act_loc],
                "Activation Name": [act_name],
                "Magnitude": [mag / mags_prev[pos]],
            }
        )

        # Append the new row to the dataframe
        relative_df = pd.concat([relative_df, row], ignore_index=True)

    mags_prev = mags

relative_fig = line_plot(
    relative_df,
    log_y=False,
    title="Magnitude(n)/Magnitude(n-1) across layers n",
    legend_title_text="Token",
)

# Set y label to be "Magnitude growth rate"
relative_fig.update_yaxes(title_text="Magnitude growth rate")

# Set y bounds to [.9, 1.5]
relative_fig.update_yaxes(range=[0.9, 1.5])

# Plot a horizontal line at y=1
relative_fig.add_hline(y=1, line_dash="dash", line_color="black")

relative_fig.show()


# %%
# Print the geometric mean of the magnitude growth rates
for pos in range(6):
    pos_df: pd.DataFrame = relative_df[relative_df["Prompt"] == tokens[pos]]
    geom_avg: float = pos_df["Magnitude"].prod() ** (1 / len(pos_df))
    print(
        f"The `{tokens[pos]}` token (position {pos}) has an average growth"
        f" rate of {geom_avg:.3f}"
    )

# %% [markdown]
# The exponential increase in magnitude is confirmed, with tokens having
# an average growth rate of about 1.12. Once again, the `<|endoftext|>` token is an outlier.
