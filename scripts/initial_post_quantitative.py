# %%[markdown]
# ## Quantitative Evaluations of Activation Injection on a Language Model
#
# Reading model completions in the above qualitative results section is
# an engaging way to build some intuitions for the effects of the
# activation injections. This section compliments this by presenting
# several tools to quantity the *effectiveness* and *specificity* of the Activation
# Injection technique. Here *effectiveness* refers to the ability to change model
# behavior in the intended way (i.e. "did it work?") and *specificity*
# refers to the ability to preserve model behavior and capabilities that
# are orthogonal or unrelated to the intended change (i.e. "did we avoid
# breaking something else?")
#
# Both effectiveness and specificity are properties of the probability
# distribution from input tokens $\textbf{t^(0:k)}$ to next token
# $t^(k+1)$ that is implicitly defined by the model. An effective
# intervention would, on average, increase the probabilities of next
# tokens that are consistent with the steering goal; a specific
# intervention would minimize changes in probability associated with
# tokens that are orthogonal or unrelated to the steering goal (modulo
# any required distribution re-normalization of course). The
# non-linearity and large input space of language models makes
# it impossible to operate on these distributions directly; the best we
# can do is operate on sets of conditional distributions defined by
# paticular input texts. But how should we choose these input texts, and
# how should we operationalize the effectiveness and specificity
# criteria?
#
# (In the below expositions, we'll refer to an example steering goal of "make
# the model talk about weddings more", or just "the weddings goal" for short.)
#
# Possible input text copora:
# 1. The simplest would be to hand-select one (or a small set) of input
#    texts that we expect to produce next-token distributions that would
#    be affected by the activation injection. The resulting
#    distributions over next token in the original and modified models
#    can then be compared directly to measure the effect of the
#    intervention. For example we may pick a handful of sentences that
#    could grammatically end with the word "wedding".
# 2. A natural extension to this would be to find or generate large
#    labeled sets of "steering-goal-relevant" input texts, and evaluate
#    statistics over the set of resulting next-token distributions and
#    input labels. For example, we could mine the text of wedding
#    magazines, split into sentences, and look at the next-token
#    distributions when conditioned on the sentence fragments before the
#    first wedding-related token.
# 3. Finally, we could use completions generated by the normal and
#    modified models themselves as the input texts, and manually or
#    automatically label these generated texts using metrics that derive
#    from the steering goals. For example, we could use a metric
#    that simply counts occurences of a predefined set of
#    "wedding-related" tokens and labels each completion accord to the
#    total count. Both the next-token distributions at the corresponding
#    positions and the labels themselves can be used as evaluation tools
#    (see below for more details.)
#
# Assuming we have the full (conditional) distribution over next
# tokens determined (i.e. we have the final-position logits of the
# model), we define *effectiveness* as the total increase is probability
# of the
# set of "steering-positive" tokens from the original to the modified
# model, minus the total increase in
# probabilty of the set of "steering-negative" tokens, where
# "steering-positive" tokens are those that are congruent with the
# steering goal given the input text, and "steering-negative" tokens
# are those that are opposed to the goal. For example,
# "steering-positve" tokens might be a pre-defined set of
# wedding-related tokens. All other tokens are
# considered "steering-neutral". Either steering-positive or
# steering-negative token sets can be empty. In some contexts these sets
# can be registered explicitly a priori (e.g. the weddings example),
# which is clearly preferred to
# avoid researcher bias; however, for subtler steering goals this isn't
# possible and the steering-set of a token needs to be assigned
# in-context.
#
# The metric we use for *specificity* on a next-token distribution is
# the KL divergence from the original model to the modified model. A key
# assumption here is that most tokens fall into the "steering-neutral"
# set, and thus the desired steering effect on the probabilities of a
# small set of key non-neutral tokens will have a negligible impact on
# the KL divergence over the full distributions.  In other words,
# effectiveness does come with a specificity cost according to this
# metric, but we expect this to be negligible in practice.
#
# In addition to these distribution-based metrics, in case (3) above
# where we are generating texts as model completions, we can use the
# implicitly labels "from original model" and "from modified model" to
# assess effectiveness and specificity. Effectivness can be defined as
# the increasse in the mean value of a steering-relevant label in
# modified-model completions vs original-model completions. Specificity
# can be defined as the increase in average loss on the original model
# for the modified-model completions vs the original-model completions.
# As above, this specificity metric will penalize effectiveness, as the
# changes made towards the steering goal will on average result in
# higher loss on the original model; once again, we expect this effect
# to be negligible in practice.
#
# We'll present results from all of these quantification approaches
# applied to a handful of steering goals in the following subsections,
# starting with the "weddings" example.  But first the usual imports,
# etc. and loading GPT2-XL.

# %%
# Imports, etc
import pickle
import textwrap

import numpy as np
import pandas as pd
import scipy as sp
import torch
from tqdm.auto import tqdm
from IPython.display import display, Image
import plotly.express as px
import plotly.graph_objects as go
import plotly as py
import plotly.subplots

from transformer_lens import HookedTransformer

from algebraic_value_editing import (
    hook_utils,
    prompt_utils,
    utils,
    sweeps,
    metrics,
    logging,
)

utils.enable_ipython_reload()

# Disable gradients to save memory during inference
_ = torch.set_grad_enabled(False)


# %%
# Load a model
MODEL: HookedTransformer = HookedTransformer.from_pretrained(
    model_name="gpt2-xl", device="cpu"
).to("cuda:1")


# %%[markdown]
# ### Steering Goal: "talk about weddings a lot"
#
# Here we'll use a simple topic-related steering goal to demonstrate the
# evaluation tools and learn a bit about how effectiveness and
# specificity respond to different hyperparameters.  First, let's define
# some constants and helpers:

# %%
# Define prompts, etc.

# Prompts to test
SINGLE_PROMPT = ["Frozen starts off with a scene about"]
PROMPTS = [
    "I went up to my friend and said",
    "Frozen starts off with a scene about",
]

# Phrases to use as the patch input
RICH_PROMPT_PHRASES = [
    [
        (" weddings", 1.0),
        (" ", -1.0),
    ]
]

# The wedding-words-count metric
# TODO: add more metrics
METRICS_DICT = {
    "wedding_words": metrics.get_word_count_metric(
        [
            "wedding",
            "weddings",
            "wed",
            "marry",
            "married",
            "marriage",
            "bride",
            "groom",
            "honeymoon",
        ]
    ),
}

# Coefficients and layers to sweep over in the "layers-dense" sweep
ACT_NAMES_SWEEP_COEFFS = [-1, 1, 2, 4]
ACT_NAMES_SWEEP_ACT_NAMES = [
    prompt_utils.get_block_name(block_num=num)
    for num in range(0, len(MODEL.blocks), 1)
]

# Coefficients and layers to sweep over in the "coefficient-dense" sweep
COEFFS_SWEEP_COEFFS = np.linspace(0, 2, 50)
COEFFS_SWEEP_ACT_NAMES = [
    prompt_utils.get_block_name(block_num=num) for num in [0, 6, 16]
]

# Sampling parameters
SAMPLING_ARGS = dict(seed=0, temperature=1, freq_penalty=1, top_p=0.3)
NUM_NORMAL_COMPLETIONS = 100
NUM_PATCHED_COMPLETIONS = 100
TOKENS_TO_GENERATE = 40


# %%[markdown]
# Next, we'll pick a single prompt "Frozen starts off with a scene
# about", and apply our distribution-based effectiveness and specificity
# evaluations.

# %%
Image("weddings_tokens1.png")

# %%[markdown]
# --------------------------------------------------------------------------------
# ## Archive / Drafting

# %%[markdown]
# ## Quantitative Evaluations of Activation Injection on a Language Model
#
# In this section we describe a handful of quantitative evaluations
# intended to assess the *effectiveness* and *specificity* of the Activation
# Injection technique.  Here effectiveness refers to an ability to change model
# behavior in the intended way (i.e. "did it work?") and specificity
# refers to an ability to preserve model behavior and capabilities that
# are orthogonal or unrelated to the intended change (i.e. "did we avoid
# breaking something else?")
#
# We use these tools to "zoom out" and evaluate the technique over a
# range of layers, coefficients, prompts, etc. to identify patterns that
# could help understand or improve the technique. We also use similar tools to
# "zoom in" and build intuitions about how the technique works in detail
# for a few specific examples.
#
# ### Summary of Quantitative Evaluations
#
# We developed several approaches for quantative evaluations which can
# be broken down according to the data evaluated (sampled completions or
# output logits), the disiderata evaluated (effectiveness or
# specificity) and the evaluation method:
# - Completions:
#   - Effectiveness:
#       - Simple heuristics e.g. count topic-related words. Simple, fast
#         and clear, but only suitable for certain steering goals.
#       - Human ratings e.g. "rate out of 10 the happiness of this
#         text". Can evaluate nuanced steering goals, but is slow,
#         scale-limited and hard to calibrate between raters.
#       - ChatGPT ratings using similar prompts. Can evaluate somewhat
#         nuanced goals, is fast and scalable, but also has calibration
#         problems.
#   - Specificity:
#       - Loss on unmodified model. If an injection has "broken the
#         model", we'd expect completions sampled from this model to
#         have much higher loss than a control group of completions of
#         the same prompt on the original model. A challenge for this
#         metric is that a successful steering will of course result in
#         completions that are less probable for the original model, and
#         thus higher loss, even when the technique is "working". One
#         mitigation for this is to use e.g. the median per-token loss
#         rather than the mean, or otherwise remove outliers.  The
#         intuition behind this being that a capable steered model
#         should generate completions that are grammatically correct and
#         sensible despite having a less probable subject, style,
#         sentiment, etc. The "critical tokens" in a given completion whose probability is
#         significantly altered by a successful steerin are likely few
#         in number, with most tokens being primarily determined by
#         grammatical constraints or already-introduced "critical
#         tokens".  Thus, if we take the median loss, we should filter
#         out the affect of these "critical tokens" and better evaluate
#         retained capabilities.
#       - Human ratings as above, but evaluating "coherence" or similar.
#       - ChatGPT ratings as above, but evaluating "coherence" or
#         similar.
# - Logits:
#   - Effectiveness: change in probability of key token(s) at key positions in a
#     specific text sequence. This is the most "zoomed in" metric:
#     looking at a single position in a single sequence, for a small
#     number of possible tokens, and thus provides the most direct
#     and granular visibility into the effect of an injection.
#   - Specificity: KL divergence of the token distributions at key
#     positions in a specific text sequence. A "highly specific"
#     intervention would be expected to change probabilities for a small
#     number of relevant tokens, while leaving the rest of the
#     distribution relatively unchanged.
#
# We going to use an example context to introduce all of these approaches:
# a simple topic-based injection intended to steer the model towards sequences
# that are related to weddings.  After that we'll show results for a
# handful of other steering objectives.
#
# Let's begin with the usual imports and boilerplate, and also load a model:

# %%
# Imports, etc
import pickle
import textwrap

import numpy as np
import pandas as pd
import scipy as sp
import torch
from tqdm.auto import tqdm
from IPython.display import display
import plotly.express as px
import plotly.graph_objects as go
import plotly as py
import plotly.subplots

from transformer_lens import HookedTransformer

from algebraic_value_editing import (
    hook_utils,
    prompt_utils,
    utils,
    sweeps,
    metrics,
    logging,
)

utils.enable_ipython_reload()

# Disable gradients to save memory during inference
_ = torch.set_grad_enabled(False)


# %%
# Load a model
MODEL: HookedTransformer = HookedTransformer.from_pretrained(
    model_name="gpt2-xl", device="cpu"
).to("cuda:1")


# %%[markdown]
# Next, we define some parameters that will be used in this
# wedding-topic-steering example, including the metrics. Specifically:
# - The prompts that will be used to generate completions on the normal
#   and modified models.
# - The phrases, coefficients and injection layers that will be used to
#   intervene on the model.
# - The "relevant words" used in a simple word-count completion
#   effectiveness metric.
# - The prompts used to obtain human and ChatGPT effectiveness and
#   specificiy ratings.

# %%
# Define prompts, etc.

# Prompts to test
PROMPTS = [
    "I went up to my friend and said",
    "Frozen starts off with a scene about",
]

# Phrases to use as the patch input
RICH_PROMPT_PHRASES = [
    [
        (" weddings", 1.0),
        (" ", -1.0),
    ]
]

# The wedding-words-count metric
# TODO: add more metrics
METRICS_DICT = {
    "wedding_words": metrics.get_word_count_metric(
        [
            "wedding",
            "weddings",
            "wed",
            "marry",
            "married",
            "marriage",
            "bride",
            "groom",
            "honeymoon",
        ]
    ),
}

# Coefficients and layers to sweep over in the "layers-dense" sweep
ACT_NAMES_SWEEP_COEFFS = [-1, 1, 2, 4]
ACT_NAMES_SWEEP_ACT_NAMES = [
    prompt_utils.get_block_name(block_num=num)
    for num in range(0, len(MODEL.blocks), 1)
]

# Coefficients and layers to sweep over in the "coefficient-dense" sweep
COEFFS_SWEEP_COEFFS = np.linspace(0, 2, 50)
COEFFS_SWEEP_ACT_NAMES = [
    prompt_utils.get_block_name(block_num=num) for num in [0, 6, 16]
]

# Sampling parameters
SAMPLING_ARGS = dict(seed=0, temperature=1, freq_penalty=1, top_p=0.3)
NUM_NORMAL_COMPLETIONS = 100
NUM_PATCHED_COMPLETIONS = 100
TOKENS_TO_GENERATE = 40

# %%[markdown]
#
# We'll also define a convenience wrapper function to perform a sweep
# with all the required parameters that will be used several times.


# %%
def run_sweep(
    model,
    prompts,
    phrases,
    act_names,
    coeffs,
    metrics_dict,
    log,
    run_path=None,
):
    """Convenience wrapper for performing sweeps, including optional pulling of
    cached data from wandb."""
    # Always generate the RichPrompts df
    rich_prompts_df = sweeps.make_rich_prompts(
        phrases=phrases,
        act_names=act_names,
        coeffs=coeffs,
    )

    # Pull data from a cached run, if path is provided
    if run_path is not None:
        normal_df, patched_df = logging.get_objects_from_run(
            run_path, flatten=True
        )
    # Otherwise perform the run and save the results
    else:
        normal_df, patched_df = sweeps.sweep_over_prompts(
            model=model,
            prompts=prompts,
            rich_prompts=rich_prompts_df["rich_prompts"],
            num_normal_completions=NUM_NORMAL_COMPLETIONS,
            num_patched_completions=NUM_PATCHED_COMPLETIONS,
            tokens_to_generate=TOKENS_TO_GENERATE,
            metrics_dict=metrics_dict,
            log={"tags": ["initial_post"]},
            **SAMPLING_ARGS
        )
        print(logging.last_run_info)

    # Reduce data
    reduced_normal_df, reduced_patched_df = sweeps.reduce_sweep_results(
        normal_df, patched_df, rich_prompts_df
    )

    # Plot
    sweeps.plot_sweep_results(
        reduced_patched_df,
        "wedding_words_count",
        "Average wedding word count",
        col_x="act_name",
        col_color="coeff",
        baseline_data=reduced_normal_df,
    ).show()
    sweeps.plot_sweep_results(
        reduced_patched_df,
        "loss",
        "Average loss",
        col_x="act_name",
        col_color="coeff",
        baseline_data=reduced_normal_df,
    ).show()

    px.scatter(
        reduced_patched_df,
        x="wedding_words_count",
        y="loss",
        color=[int(ss.split(".")[1]) for ss in reduced_patched_df["act_name"]],
        size=reduced_patched_df["coeff"] - reduced_patched_df["coeff"].min(),
        size_max=10,
        facet_col="prompts",
        hover_data=["act_name", "coeff"],
    ).show()

    # Return data for any future use
    return reduced_normal_df, reduced_patched_df


# %%[markdown]
# With these preliminaries in place, we're ready to perform our first
# quantitative evaluation of the weddings-steering intervention. The
# question we're asking here is "how does the effectiveness and
# specificity of the weddings steering change over injection layer for a
# handful of coefficient values?"  Let's find out:

# %%
# Perform a layers-dense sweep and visualize
rich_prompts_df = sweeps.make_rich_prompts(
    phrases=RICH_PROMPT_PHRASES,
    act_names=ACT_NAMES_SWEEP_ACT_NAMES,
    coeffs=ACT_NAMES_SWEEP_COEFFS,
)

# Code to run the actual sweep
# normal_df, patched_df = sweeps.sweep_over_prompts(
#     model=MODEL,
#     prompts=PROMPTS,
#     rich_prompts=rich_prompts_df["rich_prompts"],
#     num_normal_completions=NUM_NORMAL_COMPLETIONS,
#     num_patched_completions=NUM_PATCHED_COMPLETIONS,
#     tokens_to_generate=TOKENS_TO_GENERATE,
#     metrics_dict=METRICS_DICT,
#     log={"tags": ["initial_post"], "group": "wedding-act-names-sweep"},
#     **SAMPLING_ARGS
# )

# Instead, load pre-cached from wandb
normal_df, patched_df = logging.get_objects_from_run(
    "montemac/algebraic_value_editing/6zwyi2au", flatten=True
)

# Before Weights and Biases logging we just cached results locally
# CACHE_FN = "wedding-act-names-sweep.pkl"
# try:
#     with open(CACHE_FN, "rb") as file:
#         normal_df, patched_df, rich_prompts_df = pickle.load(file)
# except FileNotFoundError:
#     normal_df, patched_df = sweeps.sweep_over_prompts(
#         model=MODEL,
#         prompts=PROMPTS,
#         rich_prompts=rich_prompts_df["rich_prompts"],
#         num_normal_completions=NUM_NORMAL_COMPLETIONS,
#         num_patched_completions=NUM_PATCHED_COMPLETIONS,
#         tokens_to_generate=TOKENS_TO_GENERATE,
#         metrics_dict=METRICS_DICT,
#         log={"tags": ["initial_post"], "group": "wedding-act-names-sweep"},
#         **SAMPLING_ARGS
#     )
#     print(logging.last_run_info)
#     with open(CACHE_FN, "wb") as file:
#         pickle.dump((normal_df, patched_df, rich_prompts_df), file)

# Reduce data
reduced_normal_df, reduced_patched_df = sweeps.reduce_sweep_results(
    normal_df, patched_df, rich_prompts_df
)

# Plot
sweeps.plot_sweep_results(
    reduced_patched_df,
    "wedding_words_count",
    "Average wedding word count",
    col_x="act_name",
    col_color="coeff",
    baseline_data=reduced_normal_df,
).show()
sweeps.plot_sweep_results(
    reduced_patched_df,
    "loss",
    "Average loss",
    col_x="act_name",
    col_color="coeff",
    baseline_data=reduced_normal_df,
).show()


px.scatter(
    reduced_patched_df,
    x="wedding_words_count",
    y="loss",
    color=[int(ss.split(".")[1]) for ss in reduced_patched_df["act_name"]],
    size=reduced_patched_df["coeff"] - reduced_patched_df["coeff"].min(),
    size_max=10,
    facet_col="prompts",
    hover_data=["act_name", "coeff"],
).show()

# %%[markdown]
# Some observations jump out immediately.  In no particular order...
#
# **Coefficients beyond 1.0 don't seem to reliably increase
# *effectiveness*, though they do seem to reliably increase loss**
# (TODO: check this with more metrics added.)
#
# (Discuss/analyze)
#
# **Effectiveness broadly decreases as the block number increases,
# though these curves are not simple or linear; loss also tends to
# descrease, but not monotonically.**
#
# At least for the studied prompts, it seems possible to get good
# effectiveness with negligible increase in loss by operating on layer 6
# or 7.
#
# **Negative coefficients have little effect most of the time.**  This
# is not true in general, and appears to be a property of the "negative"
# prompt being a simple space-pad rather than a token with semantic value.
#
# **Why are the patched completions showing lower loss on the original model
# in later layers for the "I went up to my friend and said" prompt,
# while still being effective?**
#
# E.g. block 8, coeff 1?  Maybe changing
# the distribution in such a way that with our sampling approach (top-P,
# temperature, etc.) we're more likely to generate low-loss completions
# or something?  Don't wory about it for now, interesting mystery...

# %%
rpi = rich_prompts_df[
    (rich_prompts_df["coeff"] == 1)
    & (rich_prompts_df["act_name"] == "blocks.8.hook_resid_pre")
].index[0]
prompt = "I went up to my friend and said"
# px.line(patched_df[patched_df["rich_prompt_index"] == rpi]['completion_index'].values)
# display(
#     patched_df[
#         (patched_df["rich_prompt_index"] == rpi)
#         & (patched_df["prompts"] == prompt)
#     ]
# )
# display(normal_df[normal_df["prompts"] == prompt])

# %%[markdown]
# We can perform a similar sweep but with high density on the coefficient
# axis, for a sampling of interesting injection layers (blocks 0, 6,
# 16).

# %%
# Perform a coeffs-dense sweep and visualize
# TODO: wrap all this in a single wrapper function
rich_prompts_df = sweeps.make_rich_prompts(
    phrases=RICH_PROMPT_PHRASES,
    act_names=COEFFS_SWEEP_ACT_NAMES,
    coeffs=COEFFS_SWEEP_COEFFS,
)

# # Code to run the actual sweep
# normal_df, patched_df = sweeps.sweep_over_prompts(
#     model=MODEL,
#     prompts=PROMPTS,
#     rich_prompts=rich_prompts_df["rich_prompts"],
#     num_normal_completions=NUM_NORMAL_COMPLETIONS,
#     num_patched_completions=NUM_PATCHED_COMPLETIONS,
#     tokens_to_generate=TOKENS_TO_GENERATE,
#     metrics_dict=METRICS_DICT,
#     log={"tags": ["initial_post"], "group": "wedding-coeffs-sweep"},
#     **SAMPLING_ARGS
# )

# Instead, load pre-cached from wandb
normal_df, patched_df = logging.get_objects_from_run(
    "montemac/algebraic_value_editing/j7b5c4bb", flatten=True
)

# Reduce data
reduced_normal_df, reduced_patched_df = sweeps.reduce_sweep_results(
    normal_df, patched_df, rich_prompts_df
)

# Plot
sweeps.plot_sweep_results(
    reduced_patched_df,
    "wedding_words_count",
    "Average wedding word count",
    col_x="coeff",
    col_color="act_name",
    baseline_data=reduced_normal_df,
).show()
sweeps.plot_sweep_results(
    reduced_patched_df,
    "loss",
    "Average loss",
    col_x="coeff",
    col_color="act_name",
    baseline_data=reduced_normal_df,
).show()


# %%[markdown]
# Now, let's repeat this basic process for some other RicHPrompts and
# prompts

# %%
# Do a bunch of other sweeps...
# TODO: post-process to add ChatGPT and human-rated metrics


# %%
# Scratchpad ---------------------------

# Generage some wedding-related sentences using ChatGPT
# completion = openai.ChatCompletion.create(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Please generate five sentences that end in the word wedding"}])
# prompts = ["  "+line.split('. ')[1] for line in completion.choices[0].message.content.split('\n')]
prompts = [
    "  The bride wore a stunning white dress with a long flowing train.",
    "  The groom's family surprised everyone with a choreographed dance routine during the reception.",
    "  The wedding was held at a beautiful seaside location, and guests enjoyed breathtaking views of the ocean.",
    "  The couple exchanged personalized vows that brought tears to everyone's eyes.",
    "  The wedding cake was a towering masterpiece, adorned with intricate sugar flowers and delicate piping.",
]


# Convenience function to run a big batch of prompts in parallel, then
# separate them out and return logits and per-token loss objects of the
# original token length of each string.  Returned objects are numpy
# arrays for later analysis convenience
def run_forward_batch(MODEL, prompts):
    logits, loss = MODEL.forward(
        prompts, return_type="both", loss_per_token=True
    )
    logits_list = []
    loss_list = []
    for idx, prompt in enumerate(prompts):
        token_len = MODEL.to_tokens(prompt).shape[1]
        logits_list.append(logits[idx, :token_len, :].detach().cpu().numpy())
        loss_list.append(loss[idx, :token_len].detach().cpu().numpy())
    return logits_list, loss_list


# Run the prompts through the model as a single batch
logits_normal, loss_normal = run_forward_batch(MODEL, prompts)

# Define the activation injection, get the hook functions
rich_prompts = list(
    prompt_utils.get_x_vector(
        prompt1=" weddings",
        prompt2="",
        coeff=1.0,
        act_name=6,
        model=MODEL,
        pad_method="tokens_right",
        custom_pad_id=MODEL.to_single_token(" "),
    ),
)
hook_fns = hook_utils.hook_fns_from_rich_prompts(
    model=MODEL,
    rich_prompts=rich_prompts,
)

# Attach hooks, run another forward pass, remove hooks
MODEL.remove_all_hook_fns()
for act_name, hook_fn in hook_fns.items():
    MODEL.add_hook(act_name, hook_fn)
logits_mod, loss_mod = run_forward_batch(MODEL, prompts)
MODEL.remove_all_hook_fns()


# Plot some stuff
def plot_ind(ind):
    df = pd.concat(
        [
            pd.DataFrame({"loss": loss_normal[ind], "model": "normal"}),
            pd.DataFrame({"loss": loss_mod[ind], "model": "modified"}),
            pd.DataFrame(
                {
                    "loss": loss_mod[ind] - loss_normal[ind],
                    "model": "modified-normal",
                }
            ),
        ]
    )
    fig = px.line(
        df,
        y="loss",
        color="model",
        title=prompts[ind],
    )
    fig.update_layout(
        xaxis=dict(
            tickmode="array",
            tickvals=np.arange(len(MODEL.to_str_tokens(prompts[ind])[1:])),
            ticktext=MODEL.to_str_tokens(prompts[ind])[1:],
        )
    )
    fig.show()


plot_ind(2)

for loss_n, loss_m in zip(loss_normal, loss_mod):
    print(loss_n.mean(), loss_m.mean())
