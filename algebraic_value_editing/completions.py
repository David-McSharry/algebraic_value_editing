""" Functions for generating completions from a model, using a prompt and a list of RichPrompts. """

import warnings
from typing import List, Union, Optional

import torch
import torch.nn.functional
import pandas as pd
import prettytable

from transformer_lens.hook_points import (
    HookedModel,
)

from algebraic_value_editing.rich_prompts import RichPrompt
from algebraic_value_editing.hook_utils import get_prompt_hook_fns


def prompt_to_tokens(model: HookedModel, prompt: Union[str, List[str]]) -> torch.Tensor:
    """Converts a prompt (or batch of prompts) to a tensor of tokens."""
    if isinstance(prompt, str):
        prompt = [prompt]
    return model.to_tokens(prompt)  # NOTE is this torch.Tensor?


def complete_prompt_normal(
    model: HookedModel,
    prompt: Union[str, List[str]],
    completion_length: int = 40,
    seed: Optional[int] = None,
    **sampling_kwargs,
):
    """Completes a prompt (or batch of prompts) using the model's generate function.

    Removes all hooks from the model before generating completions.
    """
    target_tokens = prompt_to_tokens(model, prompt)

    if seed is not None:
        torch.manual_seed(seed)

    # Get the completions
    model.remove_all_hook_fns()
    completion = model.generate(
        target_tokens,
        max_new_tokens=completion_length,
        verbose=False,
        **sampling_kwargs,
    )
    loss = (
        model(completion, return_type="loss", loss_per_token=True)
        .mean(axis=1)
        .detach()
        .cpu()
        .numpy()
    )

    if seed is not None:
        torch.manual_seed(torch.initial_seed())

    # Convert to strings, excluding the <EOS> token
    return [model.to_string(compl[1:]) for compl in completion], loss, target_tokens


def complete_prompt_with_rich_prompts(
    model: HookedModel,
    prompt: Union[str, List[str]],  # NOTE maybe this should be removed?
    rich_prompts: List[RichPrompt],
    completion_length: int = 40,
    control_type: Optional[str] = None,  # NOTE not yet implemented
    seed: Optional[int] = None,  # Random seed to use for generation
    include_normal: bool = True,  # Whether to generate completions from the unmodified model
    **sampling_kwargs,  # Keyword arguments to pass to the model's generate function
) -> pd.DataFrame:
    """Compare the model with and without hooks generated by the rich_prompts.
    The hooks are specified by a list of ((promptA, promptB), coefficient) tuples, which creates a net x-vector, or a
    pre-calculated x-vector can be passed instead.  If control_type is not None, it should be a string specifying
    a type of control to use (currently only 'randn' is supported).

    Returns a tuple of completions as Tensors of tokens, with control completion included if control_type is set.
    """
    # Get the normal completions if requested
    if include_normal:
        normal_completion_str, normal_loss, target_tokens = complete_prompt_normal(
            model, prompt, completion_length, seed, **sampling_kwargs
        )

    # Patch the model
    # act_name = utils.get_act_name("resid_pre", block_num) # TODO delete
    hook_fns = get_prompt_hook_fns(model=model, rich_prompts=rich_prompts)
    for act_name, x_vector_fn in hook_fns.items():
        model.add_hook_fn(act_name, x_vector_fn)

    # Set seeds
    if seed is not None:
        torch.manual_seed(seed)

    # Run the patched model
    patched_completion = model.generate(
        target_tokens,
        max_new_tokens=completion_length,
        verbose=False,
        **sampling_kwargs,
    )
    patched_loss = (
        model(patched_completion, return_type="loss", loss_per_token=True)
        .mean(axis=1)
        .detach()
        .cpu()
        .numpy()
    )
    model.remove_all_hook_fns()

    # Set seeds again
    if seed is not None:
        torch.manual_seed(seed)  # TODO set it back to original seed?

    # Run a control-patched model if desired
    if control_type == "randn":
        warnings.warn("Control not supported yet")
    model.remove_all_hook_fns()

    # Put the completions into a DataFrame and return
    results = pd.DataFrame(
        {
            "prompt": prompt,
            "patched_completion": [model.to_string(compl[1:]) for compl in patched_completion],
            "patched_loss": patched_loss,
        }
    )

    if include_normal:
        results["normal_completion"] = normal_completion_str
        results["normal_loss"] = normal_loss

    return results


def bold_text(text: str) -> str:
    """Returns a string with ANSI bold formatting."""
    return f"\033[1m{text}\033[0m"


def print_n_comparisons(prompt: str, num_comparisons: int = 5, **kwargs):
    """Pretty-print num_comparisons generations from patched and unpatched. Takes parameters for complete_prompt_with_rich_prompts.
    """
    # Generate the table
    table = prettytable.PrettyTable()
    table.align = "l"
    table.field_names = map(bold_text, ["Patched completion", "Normal completion"])

    # Ensure text has appropriate width
    width = 60
    table.min_width = {fname: width for fname in table.field_names}
    table.max_width = {fname: width for fname in table.field_names}
    # Separate completions
    table.hrules = prettytable.ALL

    # Create the repeated prompt list
    prompts_list = [prompt] * num_comparisons

    # Get the completions
    results = complete_prompt_with_rich_prompts(prompt=prompts_list, **kwargs)

    # Formatting function
    def apply_formatting(str_):
        completion = "".join(str_.split(prompt)[1:])
        return f"{bold_text(prompt)}{completion}"

    # Put into table
    for rr, row in results.iterrows():  # TODO what is this doing
        patch_str = apply_formatting(row["patched_completion"])
        normal_str = apply_formatting(row["normal_completion"])
        table.add_row([patch_str, normal_str])

    print(table)
