""" Functions for generating completions from a model, using a prompt
and a list of RichPrompts. """

import warnings
from typing import List, Union, Optional, Tuple, Dict, Callable

import torch
import torch.nn.functional
import numpy as np
import pandas as pd
import prettytable

from transformer_lens import HookedTransformer

from algebraic_value_editing.rich_prompts import RichPrompt
from algebraic_value_editing.hook_utils import get_prompt_hook_fns


def prompt_to_tokens(
    model: HookedTransformer, prompt: Union[str, List[str]]
) -> torch.Tensor:
    """Converts a prompt (or batch of prompts) to a tensor of tokens."""
    if isinstance(prompt, str):
        prompt = [prompt]
    return model.to_tokens(prompt)


def complete_prompt_normal(
    model: HookedTransformer,
    prompt: Union[str, List[str]],
    completion_length: int = 40,
    seed: Optional[int] = None,
    **sampling_kwargs,
) -> Tuple[List[str], np.ndarray, torch.Tensor]:
    """Completes a prompt (or batch of prompts) using the model's
    generate function.

    Removes all hooks from the model before generating completions.
    TODO remove this function and fold into gen_using_rich_prompts?
    """
    target_tokens: torch.Tensor = prompt_to_tokens(model, prompt)

    if seed is not None:
        torch.manual_seed(seed)

    # Get the completions
    model.remove_all_hook_fns()
    completion: torch.Tensor = model.generate(
        target_tokens,
        max_new_tokens=completion_length,
        verbose=False,
        **sampling_kwargs,
    )
    loss: np.ndarray = (
        model(completion.clone(), return_type="loss", loss_per_token=True)
        .mean(axis=1)
        .detach()
        .cpu()
        .numpy()
    )

    if seed is not None:
        torch.manual_seed(torch.initial_seed())

    # Convert to strings, excluding the <EOS> token
    strings: List[str] = [model.to_string(compl[1:]) for compl in completion]
    return (
        strings,
        loss,
        target_tokens,
    )


def gen_using_rich_prompts(
    model: HookedTransformer,
    prompt: Union[str, List[str]],  # NOTE maybe this should be removed?
    rich_prompts: List[RichPrompt],
    tokens_to_generate: int = 40,
    control_type: Optional[str] = None,  # NOTE not yet implemented
    seed: Optional[int] = None,  # Random seed to use for generation
    include_normal: bool = True,  # Whether to generate completions from the unmodified model
    **sampling_kwargs,  # Keyword arguments to pass to the model's generate function
) -> pd.DataFrame:
    """Compare the model with and without hooks generated by the
    rich_prompts. Returns a DataFrame with the completions and losses.

    args:
        model: The model to use for completion. prompt: The prompt to
        use for completion.

        rich_prompts: A list of RichPrompts which will generate hook
        functions.

        tokens_to_generate: The number of additional tokens to generate.

        control_type: The type of control to use. Currently not
        implemented.

        seed: A random seed to use for generation.

        include_normal: Whether to include completions from the
        unmodified model.

        sampling_kwargs: Keyword arguments to pass to the model's
        generate function.

    returns:
        A DataFrame with the completions and losses. If include_normal
        is True, also includes completions and losses from the
        unmodified model.
    """
    # Get the normal completions if requested
    if include_normal:
        (
            normal_completion_str,
            normal_loss,
            target_tokens,
        ) = complete_prompt_normal(
            model, prompt, tokens_to_generate, seed, **sampling_kwargs
        )

    # Patch the model
    hook_fns: Dict[str, Callable] = get_prompt_hook_fns(
        model=model, rich_prompts=rich_prompts
    )
    for act_name, x_vector_fn in hook_fns.items():
        model.add_hook(act_name, x_vector_fn)

    # Set seeds
    if seed is not None:
        torch.manual_seed(seed)

    # Run the patched model
    patched_completion: torch.Tensor = model.generate(
        target_tokens,
        max_new_tokens=tokens_to_generate,
        verbose=False,
        **sampling_kwargs,
    )
    patched_loss: np.ndarray = (
        model(
            patched_completion.clone(), return_type="loss", loss_per_token=True
        )
        .mean(axis=1)
        .detach()
        .cpu()
        .numpy()
    )
    model.remove_all_hook_fns()

    # Set seeds again
    if seed is not None:
        torch.manual_seed(seed)  # TODO set it back to original seed?

    # Run a control-patched model if desired
    if control_type == "randn":
        warnings.warn("Control not supported yet")
    model.remove_all_hook_fns()

    # Put the completions into a DataFrame and return
    results = pd.DataFrame(
        {
            "prompt": prompt,
            "patched_completion": [
                model.to_string(compl[1:]) for compl in patched_completion
            ],
            "patched_loss": patched_loss,
        }
    )

    if include_normal:
        results["normal_completion"] = normal_completion_str
        results["normal_loss"] = normal_loss

    return results


def bold_text(text: str) -> str:
    """Returns a string with ANSI bold formatting."""
    return f"\033[1m{text}\033[0m"


def print_n_comparisons(
    prompt: str,
    num_comparisons: int = 5,
    include_normal: bool = True,
    **kwargs,
) -> None:
    """Pretty-print generations from the modified and unmodified models.
    Takes keyword arguments for gen_using_rich_prompts.

    args:
        prompt: The prompt to use for completion.

        num_comparisons: The number of comparisons to make.

        include_normal: Whether to include completions from the
        unmodified model.

        kwargs: Keyword arguments to pass to
        gen_using_rich_prompts.
    """
    # Generate the table
    table = prettytable.PrettyTable()
    table.align = "l"
    field_names: List[str] = []
    if include_normal:
        field_names.append("Normal completion")
    field_names.append("Patched completion")
    table.field_names = map(bold_text, field_names)
    table.min_width = table.max_width = 60
    # Separate completions
    table.hrules = prettytable.ALL

    # Create the repeated prompt list
    prompts_list: List[str] = [prompt] * num_comparisons

    # Get the completions
    results: pd.DataFrame = gen_using_rich_prompts(
        prompt=prompts_list, include_normal=include_normal, **kwargs
    )

    def apply_formatting(unformatted_str: str) -> str:
        completion: str = "".join(
            unformatted_str.split(prompt)[1:]
        )  # Remove the prompt
        return f"{bold_text(prompt)}{completion}"

    # Put into table
    for _, row in results.iterrows():
        patch_str: str = apply_formatting(row["patched_completion"])
        new_row: List[str] = []
        if include_normal:
            new_row.append(apply_formatting(row["normal_completion"]))
        new_row.append(patch_str)
        table.add_row(new_row)

    print(table)
